from dotenv import load_dotenv
load_dotenv()
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))  # å°† backend ç›®å½•åŠ å…¥ sys.path
import os
import re
import json
import uuid
import logging
from pathlib import Path
from typing import List
from fastapi import FastAPI, File, UploadFile, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response
import httpx


# ======================
# æ–°å¢ï¼šå‘é‡æ£€ç´¢ä¾èµ–
# ======================
try:
    from sentence_transformers import SentenceTransformer
    import faiss
    import numpy as np
except ImportError:
    raise ImportError("è¯·å®‰è£… RAG ä¾èµ–: pip install sentence-transformers faiss-cpu")

# ======================
# æ–°å¢ï¼šé…ç½®æ—¥å¿—
# ======================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# ======================
# PaddleOCR ç¦»çº¿é…ç½®
# ======================
os.environ["PADDLEOCR_HOME"] = "E:/documap/.paddleocr_cache"
DET_MODEL_DIR = r"E:\documap\models\paddleocr\ch_PP-OCRv4_det_infer"
REC_MODEL_DIR = r"E:\documap\models\paddleocr\ch_PP-OCRv4_rec_infer"
CLS_MODEL_DIR = r"E:\documap\models\paddleocr\ch_ppocr_mobile_v2.0_cls_infer"

app = FastAPI(title="DocMind Pro", version="2.0")

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ä¸Šä¼ ç›®å½•
UPLOAD_DIR = Path("uploads")
UPLOAD_DIR.mkdir(exist_ok=True)

# æ ¡éªŒé˜¿é‡Œäº‘API Key
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")
if not DASHSCOPE_API_KEY:
    raise ValueError("è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® DASHSCOPE_API_KEY")

# ======================
# å…¨å±€æ–‡æœ¬å­˜å‚¨
# ======================
FILE_TEXT_STORE = {}  # {file_id: {"text": str, "chunks": List[str], "keywords": List[str]}}

# ======================
# å‘é‡å­˜å‚¨ç±»
# ======================
class VectorStore:
    def __init__(self):
        try:
            local_model_path = "E:/documap/models/bge-small-zh"
            self.model = SentenceTransformer(local_model_path)
            logger.info(f"âœ… å‘é‡æ¨¡å‹åŠ è½½æˆåŠŸ: {local_model_path}")
        except Exception as e:
            logger.error(f"âŒ å‘é‡æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", exc_info=True)
            raise RuntimeError("å‘é‡æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœ¬åœ°è·¯å¾„æˆ–ç½‘ç»œ")
        self.index = None
        self.chunks = []
        self.dim = 512

    def add_texts(self, texts: list):
        if not texts:
            return
        embeddings = self.model.encode(texts, convert_to_numpy=True)
        if self.index is None:
            self.index = faiss.IndexFlatL2(self.dim)
        self.index.add(embeddings)
        self.chunks.extend(texts)

    def search(self, query: str, k=3):
        if self.index is None or len(self.chunks) == 0:
            return []
        query_vec = self.model.encode([query], convert_to_numpy=True)
        _, indices = self.index.search(query_vec, min(k, len(self.chunks)))
        return [self.chunks[i] for i in indices[0]]

VECTOR_STORES = {}

# ======================
# âœ… æ–°å¢ï¼šå…³é”®è¯æå–å‡½æ•°
# ======================
def extract_keywords_from_text(text: str) -> List[str]:
    """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯å­—æ®µï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰"""
    keywords = []
    lines = text.split('\n')
    for i, line in enumerate(lines):
        if re.search(r'\b(?:KEYWORDS|Keywords|å…³é”®è¯)\b', line, re.IGNORECASE):
            if i + 1 < len(lines):
                kw_line = lines[i + 1].strip()
                if kw_line and len(kw_line) > 3 and not kw_line.startswith('[') and not kw_line.isdigit():
                    keywords = [k.strip() for k in re.split(r'[,ï¼Œ;ï¼›]', kw_line) if k.strip()]
                    break
    return keywords

# ======================
# æ–‡æ¡£è§£æå‡½æ•°
# ======================
def extract_text_from_pdf(pdf_path: str) -> str:
    try:
        import pdfplumber
        text = ""
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                text += (page.extract_text() or "") + "\n"
        return text.strip()
    except ImportError:
        raise HTTPException(status_code=500, detail="ç¼ºå°‘PDFè§£æä¾èµ–ï¼šè¯·æ‰§è¡Œ pip install pdfplumber")
    except Exception as e:
        logger.error(f"PDFè§£æå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=400, detail=f"PDFè§£æå¤±è´¥: {str(e)}")

def extract_text_from_image(image_path: str) -> str:
    try:
        for model_dir in [DET_MODEL_DIR, REC_MODEL_DIR, CLS_MODEL_DIR]:
            if not os.path.exists(model_dir):
                raise FileNotFoundError(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
        
        from paddleocr import PaddleOCR
        ocr = PaddleOCR(
            use_angle_cls=True,
            lang="ch",
            use_gpu=False,
            det_model_dir=DET_MODEL_DIR,
            rec_model_dir=REC_MODEL_DIR,
            cls_model_dir=CLS_MODEL_DIR,
            download_model=False,
            show_log=False
        )
        logger.info(f"å¼€å§‹è§£æå›¾ç‰‡: {image_path}")
        result = ocr.ocr(image_path, cls=True)
        text = ""
        if result and isinstance(result, list) and len(result) > 0:
            for line in result:
                if line and isinstance(line, list):
                    for word_info in line:
                        if word_info and len(word_info) >= 2:
                            text += word_info[1][0] + "\n"
        text = text.strip()
        logger.info(f"æå–çš„æ–‡æœ¬: {text if text else 'ç©º'}")
        return text
    except ImportError:
        raise HTTPException(status_code=500, detail="ç¼ºå°‘OCRä¾èµ–ï¼šè¯·æ‰§è¡Œ pip install paddleocr")
    except FileNotFoundError as e:
        logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"OCRæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {str(e)}")
    except Exception as e:
        logger.error(f"å›¾ç‰‡OCRå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=400, detail=f"å›¾ç‰‡OCRå¤±è´¥: {str(e)}")

# ======================
# âœ… æ™ºèƒ½åˆ†å—å‡½æ•°ï¼ˆä¿ç•™ï¼‰
# ======================
def smart_chunk_text(text: str) -> list:
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    chunks = []
    for para in paragraphs:
        if len(para) < 50:
            if chunks:
                chunks[-1] += " " + para
            else:
                chunks.append(para)
        else:
            chunks.append(para)
    final_chunks = []
    for chunk in chunks:
        if len(chunk) > 800:
            sentences = re.split(r'[ã€‚\.\!\?\n]', chunk)
            temp = ""
            for sent in sentences:
                if len(temp + sent) > 600:
                    if temp:
                        final_chunks.append(temp.strip())
                    temp = sent
                else:
                    temp += sent + ". "
            if temp:
                final_chunks.append(temp.strip())
        else:
            final_chunks.append(chunk)
    return [c.strip() for c in final_chunks if c.strip()]

# ======================
# âœ… RAG é—®ç­”ï¼ˆä¿ç•™ï¼‰
# ======================
async def rag_answer(question: str, context_chunks: list) -> str:
    if not context_chunks:
        return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹ã€‚"
    context = "\n".join(context_chunks)[:4000]
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ã€æ–‡æ¡£å†…å®¹ã€‘å‡†ç¡®å›ç­”é—®é¢˜ã€‚
- å¦‚æœæ–‡æ¡£ä¸­æœ‰æ˜ç¡®çš„â€œKEYWORDSâ€ã€â€œKeywordsâ€ã€â€œå…³é”®è¯â€ã€â€œABSTRACTâ€ã€â€œæ‘˜è¦â€ç­‰å­—æ®µï¼Œè¯·ç›´æ¥å¼•ç”¨å…¶å†…å®¹ã€‚
- å¦‚æœé—®é¢˜æ¶‰åŠæ–¹æ³•ã€ç»“æœã€è´¡çŒ®ç­‰ï¼Œè¯·ä»å¼•è¨€ã€æ–¹æ³•æˆ–ç»“è®ºéƒ¨åˆ†æå–ã€‚
- ä¸è¦ç¼–é€ ä¿¡æ¯ï¼›å¦‚æœæ–‡æ¡£ç¡®å®æœªæåŠï¼Œè¯·å›ç­”â€œæ–‡æ¡£ä¸­æœªæåŠæ­¤å†…å®¹â€ã€‚

ã€æ–‡æ¡£å†…å®¹ã€‘
{context}

ã€é—®é¢˜ã€‘
{question}

ã€å›ç­”ã€‘
"""
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
                headers={
                    "Authorization": f"Bearer {DASHSCOPE_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "qwen-max",
                    "input": {"messages": [{"role": "user", "content": prompt}]},
                    "parameters": {"temperature": 0.3}
                }
            )
        response.raise_for_status()
        data = response.json()
        output = data.get("output", {})
        if "choices" in output and isinstance(output["choices"], list):
            answer = output["choices"][0].get("message", {}).get("content", "")
        else:
            answer = str(output)
        return answer.strip()
    except Exception as e:
        logger.error(f"RAG å›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}")
        return f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™ï¼š{str(e)}"

# ======================
# âœ… å¯¼å…¥æ–°æ¨¡å—ï¼ˆMulti-Agent + KG + Mindmapï¼‰
# ======================
try:
    from agents.scene_router import route_scene
    from agents.agent_team import analyze_with_agents
    from knowledge.graph_builder import extract_knowledge_graph_from_text
    from knowledge.mindmap_generator import generate_mindmap_from_kg
except ImportError as e:
    logger.warning(f"éƒ¨åˆ†æ¨¡å—å¯¼å…¥å¤±è´¥: {e}ï¼Œè¯·ç¡®ä¿ agents/ å’Œ knowledge/ ç›®å½•å­˜åœ¨")
    # å…œåº•å‡½æ•°ï¼ˆé˜²æ­¢å¯¼å…¥å¤±è´¥å¯¼è‡´æœåŠ¡å¯åŠ¨å¤±è´¥ï¼‰
    async def extract_knowledge_graph_from_text(text, doc_type="general"):
        return {
            "nodes": [{"id": "root", "label": "æŠ€æœ¯å†…å®¹åˆ†æ", "type": "Category"}],
            "edges": []
        }
    
    async def generate_mindmap_from_kg(kg_dict, reasoning_steps=None):
        return {
            "root": {
                "id": "root",
                "topic": "ç³»ç»ŸçŸ¥è¯†æ¦‚è§ˆ",
                "children": [{"topic": "æš‚æ— è¯¦ç»†åˆ†æ"}]
            }
        }
    
    def route_scene(*args, **kwargs):
        return {"agent_types": ["general"]}
    
    def analyze_with_agents(text, agent_types):
        return {
            "summary": text[:1000],
            "reasoning_steps": [text[:500]]
        }

# ======================
# âœ… æ–°å¢ï¼šæ€ç»´å¯¼å›¾æ•°æ®æ ¼å¼åŒ–å‡½æ•°
# ======================
def format_mindmap_data(raw_data):
    """ç»Ÿä¸€æ ¼å¼åŒ–æ€ç»´å¯¼å›¾æ•°æ®ï¼Œç¡®ä¿ç¬¦åˆå‰ç«¯æ¸²æŸ“è¦æ±‚"""
    # åŸºç¡€æ ¡éªŒï¼šç¡®ä¿rootå­—æ®µå­˜åœ¨
    if not isinstance(raw_data, dict) or "root" not in raw_data:
        logger.warning("âš ï¸ æ€ç»´å¯¼å›¾æ•°æ®ç¼ºå°‘rootå­—æ®µï¼Œè‡ªåŠ¨è¡¥å……é»˜è®¤æ ¹èŠ‚ç‚¹")
        raw_data = {
            "root": {
                "id": "root",
                "topic": "æ–‡æ¡£åˆ†æç»“æœ",
                "children": []
            }
        }
    
    root = raw_data["root"]
    # ç¡®ä¿rootèŠ‚ç‚¹æœ‰å¿…è¦çš„å­—æ®µ
    root.setdefault("id", "root")
    root.setdefault("topic", "æ–‡æ¡£åˆ†æç»“æœ")
    root.setdefault("children", [])
    
    # é€’å½’æ ¡éªŒå­èŠ‚ç‚¹
    def validate_children(nodes, parent_id="root"):
        validated = []
        for i, node in enumerate(nodes):
            if not isinstance(node, dict):
                continue
            # ç¡®ä¿å­èŠ‚ç‚¹æœ‰idå’Œtopic
            node.setdefault("id", f"{parent_id}-{i}")
            node.setdefault("topic", f"å­èŠ‚ç‚¹{i+1}")
            node.setdefault("children", [])
            # é€’å½’æ ¡éªŒå­™å­èŠ‚ç‚¹
            node["children"] = validate_children(node["children"], node["id"])
            validated.append(node)
        return validated
    
    root["children"] = validate_children(root["children"])
    return raw_data

# ======================
# APIæ¥å£
# ======================
@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    try:
        logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file.filename}, ç±»å‹: {file.content_type}")
        file_id = str(uuid.uuid4())
        ext = file.filename.split('.')[-1].lower()
        file_path = UPLOAD_DIR / f"{file_id}.{ext}"
        
        file_content = await file.read()
        with open(file_path, "wb") as f:
            f.write(file_content)
        logger.info(f"æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")

        text = ""
        if ext in ["jpg", "jpeg", "png"]:
            text = extract_text_from_image(str(file_path))
        elif ext == "pdf":
            text = extract_text_from_pdf(str(file_path))
        elif ext in ["txt", "log"]:
            try:
                text = file_content.decode("utf-8")
            except UnicodeDecodeError:
                text = file_content.decode("gbk", errors="replace")
        else:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼ˆä»…æ”¯æŒPDF/TXT/å›¾ç‰‡ï¼‰")

        logger.info(f"æå–çš„æ–‡æœ¬é•¿åº¦: {len(text)}")
        
        keywords = extract_keywords_from_text(text)
        logger.info(f"æ£€æµ‹åˆ°å…³é”®è¯: {keywords}")
        
        chunks = smart_chunk_text(text)
        FILE_TEXT_STORE[file_id] = {
            "text": text,
            "chunks": chunks,
            "keywords": keywords
        }
        
        vs = VectorStore()
        vs.add_texts(chunks)
        VECTOR_STORES[file_id] = vs
        
        # ======================
        # âœ… å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ Multi-Agent åˆ†æ
        # ======================
        logger.info("ğŸ§  å¯åŠ¨ Multi-Agent åä½œåˆ†æ...")
        routing = route_scene(file_path, raw_text=text)
        agent_types = routing.get("agent_types", ["general"])
        agent_result = analyze_with_agents(text, agent_types)
        
        if "error" in agent_result:
            raise Exception(agent_result["error"])
        
        summary = agent_result["summary"]
        reasoning_steps = agent_result.get("reasoning_steps", [])
        logger.info("âœ… Multi-Agent åˆ†æå®Œæˆ")

        # ======================
        # âœ… æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆæ·»åŠ  await è°ƒç”¨å¼‚æ­¥å‡½æ•°ï¼‰
        # ======================
        doc_type = agent_types[0] if agent_types else "general"
        knowledge_graph = await extract_knowledge_graph_from_text(summary, doc_type=doc_type)

        # ======================
        # âœ… ç”Ÿæˆæ€ç»´å¯¼å›¾ï¼ˆæ·»åŠ æ•°æ®æ ¡éªŒå’Œæ ¼å¼åŒ–ï¼‰
        # ======================
        mindmap_data = await generate_mindmap_from_kg(knowledge_graph, reasoning_steps)
        
        # åº”ç”¨æ ¼å¼åŒ–
        mindmap_data = format_mindmap_data(mindmap_data)
        logger.info(f"ğŸ“Š æ ¼å¼åŒ–åçš„æ€ç»´å¯¼å›¾æ•°æ®: {json.dumps(mindmap_data, ensure_ascii=False)[:200]}...")

        response_data = {
            "file_id": file_id,
            "filename": file.filename,
            "mindmap": mindmap_data,
            "knowledge_graph": knowledge_graph,
            "extracted_text": text,
            "reasoning_steps": reasoning_steps  # å¯é€‰ï¼šç”¨äºè°ƒè¯•/å±•ç¤ºæ¨ç†è¿‡ç¨‹
        }
        
        logger.info("æ–‡ä»¶å¤„ç†å®Œæˆï¼Œè¿”å›å“åº”")
        return Response(
            content=json.dumps(response_data, ensure_ascii=False),
            media_type="application/json"
        )
    
    except HTTPException as e:
        logger.error(f"HTTPå¼‚å¸¸: {e.status_code} - {e.detail}")
        error_response = {
            "file_id": "",
            "filename": file.filename if 'file' in locals() else "",
            "mindmap": {
                "root": {
                    "id": "root",
                    "topic": f"å¤„ç†å¤±è´¥: {e.detail}",
                    "children": []
                }
            },
            "knowledge_graph": {"nodes": [], "edges": []},
            "extracted_text": "",
            "reasoning_steps": []
        }
        return Response(
            content=json.dumps(error_response, ensure_ascii=False),
            media_type="application/json",
            status_code=e.status_code
        )
    except Exception as e:
        logger.error(f"æœåŠ¡å†…éƒ¨é”™è¯¯: {str(e)}", exc_info=True)
        error_response = {
            "file_id": "",
            "filename": file.filename if 'file' in locals() else "",
            "mindmap": {
                "root": {
                    "id": "root",
                    "topic": f"æœåŠ¡å†…éƒ¨é”™è¯¯: {str(e)}",
                    "children": []
                }
            },
            "knowledge_graph": {"nodes": [], "edges": []},
            "extracted_text": "",
            "reasoning_steps": []
        }
        return Response(
            content=json.dumps(error_response, ensure_ascii=False),
            media_type="application/json",
            status_code=500
        )

@app.get("/ask")
async def ask_question(
    question: str = Query(..., description="ç”¨æˆ·æé—®"),
    file_id: str = Query(..., description="æ–‡ä»¶IDï¼Œæ¥è‡ª /upload è¿”å›")
):
    if file_id not in FILE_TEXT_STORE:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·å…ˆä¸Šä¼ ")
    
    if not question.strip():
        raise HTTPException(status_code=400, detail="é—®é¢˜ä¸èƒ½ä¸ºç©º")
    
    lower_q = question.lower()
    if any(trigger in lower_q for trigger in ["å…³é”®å­—", "å…³é”®è¯", "keyword", "keywords"]):
        keywords = FILE_TEXT_STORE[file_id].get("keywords", [])
        if keywords:
            return {"answer": ", ".join(keywords)}
        else:
            return {"answer": "æ–‡æ¡£ä¸­æœªæåŠæ­¤å†…å®¹ã€‚"}
    
    vs = VECTOR_STORES.get(file_id)
    if vs is None:
        chunks = FILE_TEXT_STORE[file_id]["chunks"]
        relevant_chunks = chunks[:3]
    else:
        relevant_chunks = vs.search(question, k=3)
    
    logger.info(f"æ£€ç´¢åˆ° {len(relevant_chunks)} ä¸ªç›¸å…³ç‰‡æ®µ")
    answer = await rag_answer(question, relevant_chunks)
    return {"answer": answer}

@app.get("/")
async def health_check():
    return {"status": "success", "message": "DocMind Pro åç«¯è¿è¡Œä¸­"}

# ======================
# å¯åŠ¨å…¥å£ï¼ˆç”¨äºæœ¬åœ°è°ƒè¯•ï¼‰
# ======================
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)