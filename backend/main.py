from dotenv import load_dotenv
import os
# [FIX] Allow duplicate OpenMP libraries (MKL/Torch/Paddle conflict) - MUST BE FIRST
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# è·å–é¡¹ç›®æ ¹ç›®å½•
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(BASE_DIR)

# åŠ è½½æ ¹ç›®å½•çš„.envæ–‡ä»¶
load_dotenv(os.path.join(PROJECT_ROOT, '.env'))
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))  # å°† backend ç›®å½•åŠ å…¥ sys.path
import os
import re
import json
import uuid
import logging
import time
import asyncio # [FIX] Add missing asyncio

from pathlib import Path
from typing import List, Dict, Any, Optional # [FIX] Added Optional
from fastapi import FastAPI, File, UploadFile, HTTPException, Query, Depends, Body, BackgroundTasks, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response
import httpx
import numpy as np # [FIX] Add numpy import for vector math
from sqlalchemy.orm import Session
from pydantic import BaseModel
from connection_manager import ConnectionManager  # [NEW]

# ======================
# å¯¼å…¥æ ¸å¿ƒä¾èµ–
# ======================
import logging
from logging import StreamHandler
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[StreamHandler()]
)
logger = logging.getLogger(__name__)

# å¯¼å…¥æ•°æ®åº“ç›¸å…³æ¨¡å—
from database import engine, Base, SessionLocal, get_db
from models import AnalysisHistory, FileTextStore
from models.qa_history import QAHistory
from models.feedback import Feedback
from models.teacher_rule import TeacherRule # [NEW]
from models.graph import GlobalNode, GlobalEdge # [NEW] Graph Models
from services.evaluator import evaluate_rag_response

# å¯¼å…¥æœåŠ¡æ¨¡å—
from services.video_processor import VideoProcessor
from services.pdf_service import pdf_service
from services.ocr_service import ocr_service
from services.llm import simple_llm
from routes import dashboard, comparison, learning, graph, review # [NEW] Import new routers

# åˆ›å»ºæ‰€æœ‰æ•°æ®åº“è¡¨ï¼ˆç¡®ä¿åœ¨å¯¼å…¥æ‰€æœ‰æ¨¡å‹åæ‰§è¡Œï¼‰
Base.metadata.create_all(bind=engine)


# å¯¼å…¥å‘é‡æ£€ç´¢ä¾èµ– - ç§»è‡³ services/vector_service.py


# åˆå§‹åŒ–è§†é¢‘å¤„ç†å™¨
try:
    video_processor = VideoProcessor()
except ImportError as e:
    logger.warning(f"è§†é¢‘å¤„ç†æ¨¡å—å¯¼å…¥å¤±è´¥: {e}ï¼Œè¯·ç¡®ä¿ services/video_processor.py å­˜åœ¨")
    video_processor = None

    video_processor = None

app = FastAPI(title="DocMind Pro", version="2.0")

# [NEW] åˆå§‹åŒ– WebSocket è¿æ¥ç®¡ç†å™¨
manager = ConnectionManager()

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# [NEW] Register Routers
app.include_router(dashboard.router)
app.include_router(comparison.router)
app.include_router(learning.router)
app.include_router(graph.router)
app.include_router(review.router) # [NEW]

# ======================
# ç›®å½•é…ç½®
# ======================
PROJECT_ROOT = Path(__file__).parent.parent
UPLOAD_DIR = PROJECT_ROOT / "uploads"
DOWNLOAD_DIR = PROJECT_ROOT / "downloads"

# åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
UPLOAD_DIR.mkdir(exist_ok=True)
DOWNLOAD_DIR.mkdir(exist_ok=True)

# æ ¡éªŒé˜¿é‡Œäº‘API Keyï¼ˆå¯é€‰ï¼Œä»…ç”¨äºRAGåŠŸèƒ½ï¼‰
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")
if not DASHSCOPE_API_KEY:
    logger.warning("æœªé…ç½® DASHSCOPE_API_KEYï¼ŒRAGåŠŸèƒ½å°†ä¸å¯ç”¨ï¼Œä½†å­¦ä¹ ç³»ç»ŸåŠŸèƒ½ä»ç„¶å¯ç”¨")

# ======================
# å…¨å±€å­˜å‚¨
# ======================
# æ³¨æ„ï¼šFILE_TEXT_STORE å’Œ ANALYSIS_HISTORY ç°åœ¨ä»æ•°æ®åº“è·å–
# VECTOR_STORES ä»ç„¶ä½¿ç”¨å†…å­˜å­˜å‚¨ï¼ˆå› ä¸ºå‘é‡ç´¢å¼•ä¸é€‚åˆæŒä¹…åŒ–åˆ°SQLiteï¼‰
from services.vector_service import VECTOR_STORES, GLOBAL_HISTORY_STORE, VectorStore # [REFACTORED]

# æ¸…ç†è¿‡æœŸæ–‡ä»¶çš„æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰
CLEANUP_INTERVAL = 3600  # 1å°æ—¶
# æ–‡ä»¶ä¿ç•™æ—¶é—´ï¼ˆç§’ï¼‰
FILE_RETENTION_TIME = 86400  # 24å°æ—¶

# ======================
# æ–‡ä»¶æ¸…ç†å‡½æ•°
# ======================
async def cleanup_expired_files():
    """æ¸…ç†è¿‡æœŸçš„ä¸Šä¼ æ–‡ä»¶å’Œç›¸å…³æ•°æ®"""
    import shutil
    current_time = time.time()
    
    # ä½¿ç”¨æ•°æ®åº“ä¼šè¯
    db = SessionLocal()
    try:
        # æ‰¾å‡ºè¿‡æœŸçš„æ–‡ä»¶è®°å½•
        expired_files = db.query(FileTextStore).filter(
            FileTextStore.upload_time < current_time - FILE_RETENTION_TIME
        ).all()
        
        expired_file_ids = [file.file_id for file in expired_files]
        
        # æ¸…ç†ç›¸å…³èµ„æº
        for file_id in expired_file_ids:
            # æ¸…ç†æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ–‡ä»¶
            for ext in ["pdf", "txt", "log", "jpg", "jpeg", "png"]:
                file_path = os.path.join(UPLOAD_DIR, f"{file_id}.{ext}")
                if os.path.exists(file_path):
                    try:
                        os.remove(file_path)
                        logger.info(f"å·²åˆ é™¤è¿‡æœŸæ–‡ä»¶: {file_path}")
                    except Exception as e:
                        logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
            
            # ä»æ•°æ®åº“åˆ é™¤æ–‡ä»¶è®°å½•
            db.query(FileTextStore).filter(FileTextStore.file_id == file_id).delete()
            
            # æ¸…ç†å‘é‡å­˜å‚¨
            if file_id in VECTOR_STORES:
                del VECTOR_STORES[file_id]
        
        # æäº¤æ•°æ®åº“æ›´æ”¹
        db.commit()
        logger.info(f"è¿‡æœŸæ–‡ä»¶æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç† {len(expired_file_ids)} ä¸ªæ–‡ä»¶")
    except Exception as e:
        logger.error(f"æ¸…ç†è¿‡æœŸæ–‡ä»¶å¤±è´¥: {str(e)}")
        db.rollback()
    finally:
        db.close()

# å®šæœŸæ¸…ç†ä»»åŠ¡
import asyncio

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶çš„åˆå§‹åŒ–ä»»åŠ¡"""
    # åˆ›å»ºæ•°æ®åº“è¡¨
    Base.metadata.create_all(bind=engine)
    logger.info("æ•°æ®åº“è¡¨åˆ›å»ºå®Œæˆ")
    
    # ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
    UPLOAD_DIR.mkdir(exist_ok=True)
    logger.info("DocMind Pro åç«¯å¯åŠ¨æˆåŠŸ")
    # å¯åŠ¨å®šæœŸæ¸…ç†ä»»åŠ¡
    asyncio.create_task(periodic_cleanup())

async def periodic_cleanup():
    """å®šæœŸæ¸…ç†è¿‡æœŸæ–‡ä»¶"""
    while True:
        await cleanup_expired_files()
        await asyncio.sleep(CLEANUP_INTERVAL)

# ======================
# å‘é‡å­˜å‚¨ç±»
# ======================
# VectorStore class moved to services/vector_service.py
# VECTOR_STORES and GLOBAL_HISTORY_STORE imported above

# ======================
# è¾…åŠ©å‡½æ•°ï¼šä¿å­˜æ–‡ä»¶åˆ°æ•°æ®åº“
# ======================
def save_file_to_db(file_id: str, filename: str, text: str = "", chunks: list = None, keywords: list = None, layout_data: list = None, upload_time: float = None) -> None:
    """
    å°†æ–‡ä»¶ä¿¡æ¯ä¿å­˜åˆ°æ•°æ®åº“
    
    Args:
        file_id: æ–‡ä»¶å”¯ä¸€æ ‡è¯†ç¬¦
        filename: åŸå§‹æ–‡ä»¶å
        text: æå–çš„æ–‡æœ¬å†…å®¹ï¼ˆé»˜è®¤ä¸ºç©ºï¼‰
        chunks: æ–‡æœ¬åˆ†å—åˆ—è¡¨ï¼ˆé»˜è®¤ä¸ºç©ºï¼‰
        keywords: å…³é”®è¯åˆ—è¡¨ï¼ˆé»˜è®¤ä¸ºç©ºï¼‰
        upload_time: ä¸Šä¼ æ—¶é—´ï¼ˆé»˜è®¤ä¸ºå½“å‰æ—¶é—´ï¼‰
    """
    db = SessionLocal()
    try:
        # è®¾ç½®é»˜è®¤å€¼
        chunks = chunks or []
        keywords = keywords or []
        layout_data = layout_data or [] # [NEW]
        upload_time = upload_time or time.time()
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        existing_file = db.query(FileTextStore).filter(FileTextStore.file_id == file_id).first()
        if existing_file:
            # æ›´æ–°ç°æœ‰æ–‡ä»¶
            existing_file.original_filename = filename
            existing_file.text = text
            existing_file.set_chunks_list(chunks)
            existing_file.keywords = json.dumps(keywords, ensure_ascii=False)
            existing_file.layout_info = layout_data # [NEW]
            existing_file.upload_time = upload_time
        else:
            # åˆ›å»ºæ–°æ–‡ä»¶è®°å½•
            file_record = FileTextStore(
                file_id=file_id,
                original_filename=filename,
                text=text,
                chunks=json.dumps(chunks, ensure_ascii=False),
                keywords=json.dumps(keywords, ensure_ascii=False),
                layout_data=json.dumps(layout_data, ensure_ascii=False), # [NEW]
                upload_time=upload_time
            )
            db.add(file_record)
        db.commit()
        logger.info(f"æ–‡ä»¶ä¿¡æ¯å·²ä¿å­˜åˆ°æ•°æ®åº“: {file_id}")
    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡ä»¶ä¿¡æ¯åˆ°æ•°æ®åº“å¤±è´¥: {str(e)}")
        db.rollback()
        raise
    finally:
        db.close()

# ======================
# âœ… æ–°å¢ï¼šå…³é”®è¯æå–å‡½æ•°
# ======================
def extract_keywords_from_text(text: str) -> List[str]:
    """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯å­—æ®µï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰"""
    keywords = []
    lines = text.split('\n')
    for i, line in enumerate(lines):
        if re.search(r'\b(?:KEYWORDS|Keywords|å…³é”®è¯)\b', line, re.IGNORECASE):
            if i + 1 < len(lines):
                kw_line = lines[i + 1].strip()
                if kw_line and len(kw_line) > 3 and not kw_line.startswith('[') and not kw_line.isdigit():
                    keywords = [k.strip() for k in re.split(r'[,ï¼Œ;ï¼›]', kw_line) if k.strip()]
                    break
    return keywords

# ======================
# æ–‡æ¡£è§£æå‡½æ•°
# ======================
def extract_text_from_pdf(pdf_path: str) -> dict:
    """è§£æPDFï¼Œè¿”å› {text, layout}"""
    try:
        import pdfplumber
        text = ""
        layout_data = [] # [{"page": 1, "words": [...]}]
        
        with pdfplumber.open(pdf_path) as pdf:
            for i, page in enumerate(pdf.pages):
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
                
                # Extract words with coordinates
                words = page.extract_words()
                # words structure: [{x0, top, x1, bottom, text}, ...]
                layout_data.append({
                    "page": i + 1,
                    "width": page.width, # [NEW]
                    "height": page.height, # [NEW]
                    "words": words
                })
                
        return {"text": text.strip(), "layout": layout_data}
    except ImportError:
        raise HTTPException(status_code=500, detail="ç¼ºå°‘PDFè§£æä¾èµ–ï¼šè¯·æ‰§è¡Œ pip install pdfplumber")
    except Exception as e:
        logger.error(f"PDFè§£æå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=400, detail=f"PDFè§£æå¤±è´¥: {str(e)}")

def extract_text_from_image(image_path: str) -> dict:
    try:
        logger.info(f"å¼€å§‹è§£æå›¾ç‰‡: {image_path}")
        # Now returns dict
        result = ocr_service.extract_text(image_path)
        logger.info(f"æå–çš„æ–‡æœ¬: {result['text'][:50]}..." if result['text'] else "ç©º")
        return result
    except FileNotFoundError as e:
        logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"OCRæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {str(e)}")
    except ImportError as e:
        raise HTTPException(status_code=500, detail="ç¼ºå°‘OCRä¾èµ–ï¼šè¯·æ‰§è¡Œ pip install paddleocr paddlepaddle")
    except Exception as e:
        logger.error(f"å›¾ç‰‡OCRå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=400, detail=f"å›¾ç‰‡OCRå¤±è´¥: {str(e)}")

# ======================
# âœ… æ™ºèƒ½åˆ†å—å‡½æ•°ï¼ˆä¿ç•™ï¼‰
# ======================
def smart_chunk_text(text: str) -> list:
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    chunks = []
    for para in paragraphs:
        if len(para) < 50:
            if chunks:
                chunks[-1] += " " + para
            else:
                chunks.append(para)
        else:
            chunks.append(para)
    final_chunks = []
    CHUNK_SIZE = 1000  # Increased from 600 to capture more context (e.g., Log timestamp + Traceback)
    OVERLAP_SIZE = 200 # Overlap to prevent splitting context boundaries
    
    for chunk in chunks:
        if len(chunk) > 1200: # Threshold slightly larger than CHUNK_SIZE
             # Split by sentence-endings or newlines to respect structure
            sentences = re.split(r'([ã€‚\.\!\?\n])', chunk) # Keep delimiters
            # Reconstruct sentences (delimiter was split out)
            real_sentences = []
            for i in range(0, len(sentences) - 1, 2):
                real_sentences.append(sentences[i] + sentences[i+1])
            if len(sentences) % 2 == 1:
                real_sentences.append(sentences[-1])
            
            current_chunk = ""
            for sent in real_sentences:
                if len(current_chunk) + len(sent) > CHUNK_SIZE:
                    if current_chunk:
                        final_chunks.append(current_chunk.strip())
                        # Context Overlap: Keep the last bit of the previous chunk
                        # Simple overlap: keep the last OVERLAP_SIZE chars roughly? 
                        # Better: Keep the last N sentences that fit in OVERLAP_SIZE?
                        # Simplified: Just start new chunk with empty.
                        # Actually user request requires overlap. 
                        # Let's keep the last sentence if it's not too huge.
                        if len(sent) < OVERLAP_SIZE:
                             # Overlap logic: Find sentences from current_chunk that fit in OVERLAP_SIZE
                             overlap_text = ""
                             # This is complex to reverse-iterate sentences strictly.
                             # Simple heuristic: Just let the chunks be distinct for now but larger.
                             # Complex overlap implementation might break code stability.
                             # Let's stick to larger chunk size (1000) which usually covers a log block.
                             pass
                    current_chunk = sent
                else:
                    current_chunk += sent
            if current_chunk:
                final_chunks.append(current_chunk.strip())
        else:
            final_chunks.append(chunk)
    
    # Post-process: Add explicit overlap if needed, but larger chunk size is often enough.
    # To truly fix "Timestamp at top, error at bottom", we need Sliding Window.
    # Let's re-implement with a simple sliding window over words/sentences if we want overlap.
    # But for minimal risk editing: Just increasing size to 1000 is safer and effective.
    
    return [c.strip() for c in final_chunks if c.strip()]

# ======================
# âœ… RAG é—®ç­”ï¼ˆä¿ç•™ï¼‰
# ======================
async def rag_answer(question: str, context_chunks: list) -> str:
    if not context_chunks:
        return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹ã€‚"
    context = "\n".join(context_chunks)[:4000]
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ã€‚
è§„åˆ™ï¼š
1. ä¼˜å…ˆæ ¹æ®ã€æ–‡æ¡£å†…å®¹ã€‘å›ç­”é—®é¢˜ã€‚
2. ã€é‡è¦ã€‘å¦‚æœã€é—®é¢˜ã€‘ä¸­åŒ…å«â€œç³»ç»ŸæŒ‡ä»¤â€æˆ–â€œç”¨äºä¿®æ­£â€çš„ä¿¡æ¯ï¼Œè¯·**åŠ¡å¿…**ä¼˜å…ˆéµå¾ªè¯¥æŒ‡ä»¤ï¼Œå³ä½¿å®ƒä¸ã€æ–‡æ¡£å†…å®¹ã€‘å†²çªã€‚è¿™æ˜¯ç”¨æˆ·çš„æ˜¾å¼æ›´æ­£ã€‚
3. å¦‚æœæ–‡æ¡£ä¸­æœ‰æ˜ç¡®çš„â€œKEYWORDSâ€ã€â€œKeywordsâ€ã€â€œå…³é”®è¯â€ã€â€œABSTRACTâ€ã€â€œæ‘˜è¦â€ç­‰å­—æ®µï¼Œè¯·ç›´æ¥å¼•ç”¨å…¶å†…å®¹ã€‚
4. ä¸è¦ç¼–é€ ä¿¡æ¯ï¼›å¦‚æœæ–‡æ¡£ç¡®å®æœªæåŠä¸”æ— ä¿®æ­£æŒ‡ä»¤ï¼Œè¯·å›ç­”â€œæ–‡æ¡£ä¸­æœªæåŠæ­¤å†…å®¹â€ã€‚

ã€æ–‡æ¡£å†…å®¹ã€‘
{context}

ã€é—®é¢˜ã€‘
{question}

ã€å›ç­”ã€‘
"""
    try:
        import asyncio
        max_retries = 3
        retry_delay = 5  # ç§’
        response = None
        
        async with httpx.AsyncClient() as client:
            for retry in range(max_retries):
                try:
                    logger.info(f"å‘é€RAGè¯·æ±‚åˆ°Dashscope APIï¼Œé‡è¯•æ¬¡æ•°: {retry+1}/{max_retries}")
                    response = await client.post(
                        "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
                        headers={
                            "Authorization": f"Bearer {DASHSCOPE_API_KEY}",
                            "Content-Type": "application/json"
                        },
                        json={
                            "model": "qwen-max",
                            "input": {"messages": [{"role": "user", "content": prompt}]},
                            "parameters": {
                                "temperature": 0.3,
                                "result_format": "message"
                            }
                        },
                        timeout=120.0  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°120ç§’
                    )
                    response.raise_for_status()
                    logger.info(f"æˆåŠŸæ”¶åˆ°RAG APIå“åº”")
                    break  # æˆåŠŸï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                except httpx.TimeoutException:
                    logger.warning(f"RAGè¯·æ±‚è¶…æ—¶ï¼Œ{retry+1}/{max_retries}ï¼Œå°†åœ¨{retry_delay}ç§’åé‡è¯•...")
                    if retry < max_retries - 1:
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 1.5  # æŒ‡æ•°é€€é¿
                    else:
                        logger.error(f"æ‰€æœ‰RAGè¯·æ±‚é‡è¯•éƒ½å¤±è´¥äº†ï¼Œè¯·æ±‚è¶…æ—¶")
                        return "ç”Ÿæˆç­”æ¡ˆæ—¶è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚"
                except httpx.RequestError as e:
                    logger.warning(f"RAGè¯·æ±‚å¤±è´¥ï¼Œ{retry+1}/{max_retries}ï¼Œé”™è¯¯: {e}ï¼Œå°†åœ¨{retry_delay}ç§’åé‡è¯•...")
                    if retry < max_retries - 1:
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 1.5  # æŒ‡æ•°é€€é¿
                    else:
                        logger.error(f"æ‰€æœ‰RAGè¯·æ±‚é‡è¯•éƒ½å¤±è´¥äº†ï¼Œé”™è¯¯: {e}")
                        return f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™ï¼š{str(e)}"
        
        data = response.json()
        output = data.get("output", {})
        if "choices" in output and isinstance(output["choices"], list):
            answer = output["choices"][0].get("message", {}).get("content", "")
        elif "text" in output:
            answer = output["text"]
        else:
            answer = str(output)
        return answer.strip()
    except Exception as e:
        logger.error(f"RAG å›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}")
        return f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™ï¼š{str(e)}"
from utils.text_parser import parse_markdown_mindmap # [NEW] Import parser


# ======================
# âœ… å¯¼å…¥æ–°æ¨¡å—ï¼ˆMulti-Agent + KG + Mindmapï¼‰
# ======================
try:
    from agents.scene_router import route_scene
    from agents.agent_team import analyze_with_agents
    import agents.agent_team
    logger.info(f"ğŸ” [DEBUG] agent_team loaded from: {agents.agent_team.__file__}")
    from knowledge.graph_builder import extract_knowledge_graph_from_text
    from knowledge.mindmap_generator import generate_mindmap_from_kg
except ImportError as e:
    logger.warning(f"éƒ¨åˆ†æ¨¡å—å¯¼å…¥å¤±è´¥: {e}ï¼Œè¯·ç¡®ä¿ agents/ å’Œ knowledge/ ç›®å½•å­˜åœ¨")
    # å…œåº•å‡½æ•°ï¼ˆé˜²æ­¢å¯¼å…¥å¤±è´¥å¯¼è‡´æœåŠ¡å¯åŠ¨å¤±è´¥ï¼‰
    async def extract_knowledge_graph_from_text(text, doc_type="general"):
        return {
            "nodes": [{"id": "root", "label": "æŠ€æœ¯å†…å®¹åˆ†æ", "type": "Category"}],
            "edges": []
        }
    
    async def generate_mindmap_from_kg(kg_dict, reasoning_steps=None):
        return {
            "root": {
                "id": "root",
                "topic": "ç³»ç»ŸçŸ¥è¯†æ¦‚è§ˆ",
                "children": [{"topic": "æš‚æ— è¯¦ç»†åˆ†æ"}]
            }
        }
    
    def route_scene(*args, **kwargs):
        return {"agent_types": ["general"]}
    
    def analyze_with_agents(text, agent_types, **kwargs):
        # Fallback always sync-compatible or appropriately async
        return {
            "summary": text[:1000] + "\n\n(ç³»ç»Ÿæç¤ºï¼šç”±äºæ¨¡å—åŠ è½½å¤±è´¥ï¼Œå·²å¯ç”¨é™çº§å¤„ç†)",
            "reasoning_steps": ["æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œè·³è¿‡æ™ºèƒ½ä½“åˆ†æ"]
        }

# ======================
# âœ… æ–°å¢ï¼šæ€ç»´å¯¼å›¾æ•°æ®æ ¼å¼åŒ–å‡½æ•°
# ======================
def format_mindmap_data(raw_data):
    """ç»Ÿä¸€æ ¼å¼åŒ–æ€ç»´å¯¼å›¾æ•°æ®ï¼Œç¡®ä¿ç¬¦åˆå‰ç«¯æ¸²æŸ“è¦æ±‚"""
    # åŸºç¡€æ ¡éªŒï¼šç¡®ä¿rootå­—æ®µå­˜åœ¨
    if not isinstance(raw_data, dict) or "root" not in raw_data:
        logger.warning("âš ï¸ æ€ç»´å¯¼å›¾æ•°æ®ç¼ºå°‘rootå­—æ®µï¼Œè‡ªåŠ¨è¡¥å……é»˜è®¤æ ¹èŠ‚ç‚¹")
        raw_data = {
            "root": {
                "id": "root",
                "topic": "æ–‡æ¡£åˆ†æç»“æœ",
                "children": []
            }
        }
    
    root = raw_data["root"]
    # ç¡®ä¿rootèŠ‚ç‚¹æœ‰å¿…è¦çš„å­—æ®µ
    root.setdefault("id", "root")
    root.setdefault("topic", "æ–‡æ¡£åˆ†æç»“æœ")
    root.setdefault("children", [])
    
    # é€’å½’æ ¡éªŒå­èŠ‚ç‚¹
    def validate_children(nodes, parent_id="root"):
        validated = []
        for i, node in enumerate(nodes):
            if not isinstance(node, dict):
                continue
            # ç¡®ä¿å­èŠ‚ç‚¹æœ‰idå’Œtopic
            node.setdefault("id", f"{parent_id}-{i}")
            node.setdefault("topic", f"å­èŠ‚ç‚¹{i+1}")
            node.setdefault("children", [])
            # é€’å½’æ ¡éªŒå­™å­èŠ‚ç‚¹
            node["children"] = validate_children(node["children"], node["id"])
            validated.append(node)
        return validated
    
    root["children"] = validate_children(root["children"])
    return raw_data

# ======================
# é™æ€æ–‡ä»¶æœåŠ¡
# ======================
from fastapi.staticfiles import StaticFiles

# é…ç½®é™æ€æ–‡ä»¶æœåŠ¡
app.mount("/static", StaticFiles(directory=str(UPLOAD_DIR)), name="static")
# é…ç½®ä¸‹è½½æ–‡ä»¶çš„é™æ€æœåŠ¡
app.mount("/downloads", StaticFiles(directory=str(DOWNLOAD_DIR)), name="downloads")

# ======================
# APIæ¥å£
# ======================

# [NEW] WebSocketEndpoint
@app.websocket("/ws/{client_id}")
async def websocket_endpoint(websocket: WebSocket, client_id: str):
    await manager.connect(websocket, client_id)
    try:
        while True:
            # ä¿æŒè¿æ¥æ´»è·ƒï¼Œä¹Ÿå¯ä»¥æ¥æ”¶å®¢æˆ·ç«¯æ¶ˆæ¯
            data = await websocket.receive_text()
            # è¿™é‡Œå¯ä»¥å¤„ç†å®¢æˆ·ç«¯å‘æ¥çš„æ¶ˆæ¯ï¼Œç›®å‰ä»…ç”¨äºä¿æ´»
            pass
    except WebSocketDisconnect:
        manager.disconnect(websocket, client_id)

@app.post("/upload")
async def upload_file(background_tasks: BackgroundTasks, file: UploadFile = File(...)):
    try:
        logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file.filename}, ç±»å‹: {file.content_type}")
        file_id = str(uuid.uuid4())
        ext = file.filename.split('.')[-1].lower()
        file_path = UPLOAD_DIR / f"{file_id}.{ext}"
        
        file_content = await file.read()
        with open(file_path, "wb") as f:
            f.write(file_content)
        logger.info(f"æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")

        # Add background task
        background_tasks.add_task(
            process_file_background, 
            file_id, 
            file_path, 
            file.filename, 
            ext, 
            file_content
        )

        return {
            "file_id": file_id,
            "filename": file.filename,
            "status": "processing",
            "message": "æ–‡ä»¶å·²ä¸Šä¼ ï¼Œæ­£åœ¨åå°åˆ†æä¸­..."
        }
    except Exception as e:
        logger.error(f"ä¸Šä¼ è¯·æ±‚å¤±è´¥: {str(e)}", exc_info=True)
        return Response(
            content=json.dumps({"error": str(e), "status": "failed"}, ensure_ascii=False),
            media_type="application/json",
            status_code=500
        )
from routes import dashboard, comparison, learning
app.include_router(dashboard.router, prefix="/api/dashboard", tags=["Dashboard"])
app.include_router(comparison.router, prefix="/api/comparison", tags=["Comparison"])
app.include_router(learning.router, prefix="/api/learning", tags=["Learning"])

async def process_file_background(file_id: str, file_path: Path, filename: str, ext: str, file_content: bytes):
    """åå°å¼‚æ­¥å¤„ç†æ–‡ä»¶åˆ†æä»»åŠ¡"""
    logger.info(f"ğŸš€ [åå°ä»»åŠ¡] å¼€å§‹åˆ†ææ–‡ä»¶: {filename} ({file_id})")
    db = SessionLocal()
    try:
        # 1. æ–‡æœ¬æå– (OCR/PDFè§£æ) & å¸ƒå±€ä¿¡æ¯
        # ======================
        text = ""
        layout_data = []
        
        if ext in ["jpg", "jpeg", "png"]:
            result = await asyncio.to_thread(extract_text_from_image, str(file_path))
            text = result["text"]
            layout_data = result["layout"]
        elif ext == "pdf":
            result = await asyncio.to_thread(extract_text_from_pdf, str(file_path))
            text = result["text"]
            layout_data = result["layout"]
        elif ext in ["txt", "log"]:
            try:
                text = file_content.decode("utf-8")
            except UnicodeDecodeError:
                text = file_content.decode("gbk", errors="replace")
            # layout for text files could be line-based if needed, but for now empty
            layout_data = [] 
        else:
            raise Exception(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}")
            
        # [FIX] Handle empty text case to avoid agent confusion
        if not text or not text.strip():
            logger.warning(f"âš ï¸ æ–‡ä»¶ {filename} å†…å®¹ä¸ºç©ºæˆ– OCR æå–å¤±è´¥")
            text = "ï¼ˆç³»ç»Ÿæç¤ºï¼šæœªèƒ½ä»æ–‡ä»¶ä¸­æå–åˆ°æœ‰æ•ˆæ–‡æœ¬ã€‚å¯èƒ½æ˜¯å›¾ç‰‡æ¨¡ç³Šã€OCRå¤±è´¥æˆ–æ–‡ä»¶ä¸ºç©ºã€‚ï¼‰"

        # [STREAM] Status Update
        await manager.broadcast({
            "type": "status_update",
            "status": "analyzing",
            "message": "æ–‡æœ¬æå–å®Œæˆï¼Œæ­£åœ¨è¿›è¡Œæ™ºèƒ½åˆ†æ...",
            "progress": 30
        }, file_id)

        logger.info(f"âœ… [åå°ä»»åŠ¡] æ–‡æœ¬æå–å®Œæˆï¼Œé•¿åº¦: {len(text)}")
        
        # 2. é¢„å¤„ç† (å…³é”®è¯/åˆ†å—)
        # ======================
        keywords = extract_keywords_from_text(text)
        chunks = smart_chunk_text(text)
        
        # æ›´æ–°æ•°æ®åº“ (åˆæ­¥ä¿å­˜)
        save_file_to_db(file_id, filename, text, chunks, keywords, layout_data)
        
        # 3. å¹¶è¡Œæ‰§è¡Œï¼šå‘é‡åŒ– (CPU/Blocking) & Multi-Agent åˆ†æ (I/O/API)
        # ======================
        logger.info("ğŸ§  [åå°ä»»åŠ¡] å¯åŠ¨å¹¶è¡Œä»»åŠ¡: å‘é‡åŒ– + Multi-Agentåˆ†æ...")

        # å®šä¹‰å‘é‡åŒ–ä»»åŠ¡
        async def run_embedding():
            vs = VectorStore() # Uses the imported class from services.vector_service
            # Prepare metadata for each chunk
            metadatas = [{"filename": filename, "file_id": file_id} for _ in chunks]
            await asyncio.to_thread(vs.add_texts, chunks, metadatas)
            VECTOR_STORES[file_id] = vs
            logger.info(f"âœ… [åå°ä»»åŠ¡] å‘é‡åŒ–å®Œæˆ (Chunks: {len(chunks)})")
            return vs

        # å®šä¹‰åˆ†æä»»åŠ¡
        async def run_agent_analysis():
            # Robustly handle route_scene (sync vs async mismatch during hot-reload)
            routing_res = route_scene(file_path, raw_text=text)
            if asyncio.iscoroutine(routing_res):
                routing = await routing_res
            else:
                routing = routing_res
            agent_types = routing.get("agent_types", ["general"])

            # [STREAM] Notify routing result
            await manager.broadcast({
                "type": "log",
                "message": f"åœºæ™¯è·¯ç”±å®Œæˆ: è¯†åˆ«ä¸º {agent_types}",
                "source": "System"
            }, file_id)
            
            # [STREAM] Define Callback
            loop = asyncio.get_running_loop()
            def agent_callback(msg):
                # msg: {'content': '...', 'name': '...', 'role': '...', 'type': '...'}
                # Check if it's a thinking message
                msg_type = msg.get("type", "agent_log")
                
                coro = manager.broadcast({
                    "type": msg_type,
                    "name": msg.get("name", "Agent"),
                    "content": msg.get("content", ""),
                    "role": msg.get("role", "assistant")
                }, file_id)
                asyncio.run_coroutine_threadsafe(coro, loop)

            import inspect
            try:
                logger.info(f"ğŸ•µï¸ [INTROSPECT] analyze_with_agents file: {inspect.getfile(analyze_with_agents)}")
                logger.info(f"ğŸ•µï¸ [INTROSPECT] analyze_with_agents sig: {inspect.signature(analyze_with_agents)}")
            except Exception as e:
                logger.error(f"Introspection failed: {e}")

            # å…³é”®ä¿®å¤ï¼šåœ¨è°ƒç”¨analyze_with_agentså‰å°±æˆªæ–­æ–‡æœ¬ï¼Œé¿å…Autogenæ¡†æ¶ç´¯ç§¯æ¶ˆæ¯åè¶…å‡ºé•¿åº¦é™åˆ¶
            # Dashscope qwen-max é™åˆ¶ä¸º30720å­—ç¬¦ï¼ŒAutogenä¼šå°†æ‰€æœ‰èŠå¤©å†å²å‘é€ç»™æ¨¡å‹
            # æ‰€ä»¥æˆ‘ä»¬éœ€è¦æ›´ä¸¥æ ¼åœ°æˆªæ–­åˆå§‹æ–‡æœ¬
            max_allowed_length = 20000  # æ›´ä¿å®ˆçš„é•¿åº¦é™åˆ¶
            truncated_text = text
            if len(truncated_text) > max_allowed_length:
                truncated_text = truncated_text[:max_allowed_length] + "\n\nï¼ˆæ–‡æœ¬è¿‡é•¿ï¼Œå·²æˆªæ–­ï¼‰"
                logger.info(f"æ–‡æœ¬å·²æˆªæ–­ï¼ŒåŸå§‹é•¿åº¦: {len(text)}, æˆªæ–­åé•¿åº¦: {len(truncated_text)}")

            # ä½¿ç”¨æˆªæ–­åçš„æ–‡æœ¬è¿›è¡Œåˆ†æ
            # [FIX] Now calling async function directly
            result = await analyze_with_agents(truncated_text, agent_types, callback=agent_callback)
            logger.info("âœ… [åå°ä»»åŠ¡] Multi-Agent åˆ†æå®Œæˆ")
            return result, agent_types

        # å¹¶è¡Œæ‰§è¡Œ
        embedding_task = asyncio.create_task(run_embedding())
        analysis_task = asyncio.create_task(run_agent_analysis())
        
        # [STREAM] Status
        await manager.broadcast({
            "type": "status_update",
            "status": "analyzing",
            "message": "å¤šæ™ºèƒ½ä½“æ­£åœ¨åä½œåˆ†æä¸­...",
            "progress": 50
        }, file_id)

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        await embedding_task
        agent_result, agent_types = await analysis_task
        
        if "error" in agent_result:
            raise Exception(agent_result["error"])
        
        summary = agent_result["summary"]
        reasoning_steps = agent_result.get("reasoning_steps", [])
        
        # [STREAM] Status
        await manager.broadcast({
            "type": "status_update",
            "status": "generating",
            "message": "åˆ†æå®Œæˆï¼Œæ­£åœ¨ç”ŸæˆçŸ¥è¯†å›¾è°±ä¸æ€ç»´å¯¼å›¾...",
            "progress": 80
        }, file_id)

        # 5. çŸ¥è¯†å›¾è°± & æ€ç»´å¯¼å›¾
        # ======================
        # åç»­é€»è¾‘ä¸å˜...
        doc_type = agent_types[0] if agent_types else "general"
        try:
            knowledge_graph = await extract_knowledge_graph_from_text(summary, doc_type=doc_type)
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†å›¾è°±ç”Ÿæˆå¤±è´¥: {str(e)}")
            knowledge_graph = {"nodes": [], "edges": []}

        # [NEW] Sync to Global Graph
        try:
            from services.graph_service import GraphService
            graph_service = GraphService(db)
            
            logger.info(f"ğŸ”„ [Global Graph] Syncing {len(knowledge_graph.get('nodes', []))} nodes to Global Brain...")
            
            for node in knowledge_graph.get("nodes", []):
                # Use label as primary key for merging, fallback to id
                node_name = node.get("label") or node.get("id")
                if node_name:
                    graph_service.add_node(
                        name=node_name, 
                        category=node.get("type", "Concept"), 
                        source_doc_id=file_id
                    )
            
            for edge in knowledge_graph.get("edges", []):
                # We assume edge source/target refer to node IDs (which we mapped to names)
                # But sometimes they might be IDs. We need to be careful.
                # In our simple extractor, usually id=label. 
                graph_service.add_edge(
                    source_name=edge.get("source"),
                    target_name=edge.get("target"),
                    relation=edge.get("relation", "related_to")
                )
            
            logger.info("âœ… [Global Graph] Integrated new knowledge into global brain")
        except Exception as ge:
            logger.error(f"âŒ [Global Graph] Sync failed: {str(ge)}", exc_info=True)

        # [NEW] ä¼˜å…ˆå°è¯•ä» Markdown è§£ææ€ç»´å¯¼å›¾ï¼ˆä¿ç•™ä¸“å®¶åŸè¯ï¼‰
        parsed_mindmap = parse_markdown_mindmap(summary)
        if parsed_mindmap:
             logger.info("âœ… [åå°ä»»åŠ¡] æˆåŠŸä» Markdown è§£ææ€ç»´å¯¼å›¾")
             mindmap_data = parsed_mindmap
        else:
             # Fallback to KG based generation
             try:
                 mindmap_data = await generate_mindmap_from_kg(knowledge_graph, reasoning_steps)
             except Exception as e:
                 logger.error(f"âŒ æ€ç»´å¯¼å›¾ç”Ÿæˆå¤±è´¥: {str(e)}")
                 mindmap_data = {"root": {"id": "root", "topic": "ç”Ÿæˆå¤±è´¥", "children": []}}
        
        mindmap_data = format_mindmap_data(mindmap_data)

        # 6. ä¿å­˜æœ€ç»ˆç»“æœ
        # ======================
        response_data = {
            "file_id": file_id,
            "filename": filename,
            "mindmap": mindmap_data,
            "knowledge_graph": knowledge_graph,
            "extracted_text": text,
            "layout_data": layout_data,
            "summary": summary,
            "reasoning_steps": reasoning_steps,
            "agent_types": agent_types, # [NEW] Pass to frontend for conditional UI
            "status": "completed" 
        }

        # æ›´æ–°/åˆ›å»ºå†å²è®°å½•
        history_entry = AnalysisHistory(
            id=str(uuid.uuid4()),
            file_id=file_id,
            filename=filename,
            analysis_time=time.time()
        )
        history_entry.result_dict = response_data
        
        db.add(history_entry)
        
        
        # [FIX] æ›´æ–° FileTextStore ä¸­çš„å…³é”®è¯
        try:
            import re
            new_keywords = []
            
            # 1. ä¼˜å…ˆå°è¯•ä» Agent è¾“å‡ºä¸­æå– "### æ ¸å¿ƒå…³é”®è¯"
            keyword_match = re.search(r'### æ ¸å¿ƒå…³é”®è¯\s*\n(.+)', summary)
            if keyword_match:
                # æå–ç¬¬ä¸€è¡Œå†…å®¹ï¼ŒæŒ‰é€—å·æˆ–é¡¿å·åˆ†å‰²
                raw_kws = re.split(r'[,ï¼Œã€]', keyword_match.group(1).strip())
                new_keywords = [k.strip() for k in raw_kws if k.strip()][:5] # Limit to 5
                logger.info(f"âœ… [åå°ä»»åŠ¡] ä» Agent æå–å…³é”®è¯: {new_keywords}")
            
            # 2. å¦‚æœæå–å¤±è´¥ï¼Œä½¿ç”¨é™çº§ç®—æ³• (Frequency-based Fallback)
            if not new_keywords:
                clean_summary = re.sub(r'[^\w\s]', ' ', summary)
                structure_stops = {
                    'æ ¸å¿ƒä¸»é¢˜', 'ç°è±¡', 'åŸå› ', 'è§£å†³æ–¹æ¡ˆ', 'å½±å“èŒƒå›´', 'æ”»å‡»æ‰‹æ®µ', 
                    'æ·±åº¦åˆ†æ', 'æ€»ç»“', 'åˆ›æ–°ç‚¹', 'æ–¹æ³•è®º', 'ç»“è®º', 'æ ‘å½¢æ€ç»´å¯¼å›¾', 
                    'æ–‡æœ¬æè¿°', 'å­èŠ‚ç‚¹', 'åˆ†æå®Œæˆ', 'terminate', 'novelty', 
                    'methodology', 'conclusion', 'chapter', 'summary', 'concepts',
                    'æ ¸å¿ƒå…³é”®è¯'
                }
                stops = {'the', 'a', 'in', 'of', 'and', 'to', 'is', 'for', 'with', 'on', 
                         'è¿™ä¸ª', 'ä¸€ä¸ª', 'å¯ä»¥', 'æˆ‘ä»¬', 'é€šå¸¸', 'ä½¿ç”¨', 'ä»¥åŠ', 'å› æ­¤', 'é€šè¿‡'}
                
                words = clean_summary.split()
                from collections import Counter
                # Exclude purely numeric or single char
                valid_words = [w for w in words if len(w) > 1 and not w.isdigit()]
                
                # Use frequency Counter to get top common words, not just first appearance
                counts = Counter(w.lower() for w in valid_words if w.lower() not in stops and w.lower() not in structure_stops)
                
                # Get top 5 most frequent
                most_common = counts.most_common(5)
                new_keywords = [word for word, count in most_common]
                
                logger.info(f"ğŸ”„ [åå°ä»»åŠ¡] å…³é”®è¯é™çº§æå–(Top5 Freq): {new_keywords}")

            file_record = db.query(FileTextStore).filter(FileTextStore.file_id == file_id).first()
            
            file_record = db.query(FileTextStore).filter(FileTextStore.file_id == file_id).first()
            if file_record:
                logger.info(f"ğŸ”„ [åå°ä»»åŠ¡] æ›´æ–°å…³é”®è¯ (Filtered): {new_keywords}")
                file_record.keywords = json.dumps(new_keywords, ensure_ascii=False)
        except Exception as kw_e:
            logger.warning(f"å…³é”®è¯æ›´æ–°å¤±è´¥: {kw_e}")

        db.commit()
        logger.info(f"âœ… [åå°ä»»åŠ¡] å…¨æµç¨‹åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜: {file_id}")

        # [NEW] WebSocket æ¨é€
        logger.info(f"ğŸš€ [Socket] Broadcasting to {file_id}. Payload sizes -> MindMap: {len(str(mindmap_data))}, KG: {len(str(knowledge_graph))}")
        await manager.broadcast(response_data, file_id)
        
        logger.info(f"ğŸ‰ [åå°ä»»åŠ¡] åˆ†æå…¨éƒ¨å®Œæˆï¼Œç»“æœå·²æ¨é€ç»™å®¢æˆ·ç«¯: {file_id}")

    except Exception as e:
        logger.error(f"âŒ [åå°ä»»åŠ¡] å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
        # æ¨é€é”™è¯¯æ¶ˆæ¯
        error_response = {"status": "failed", "error": str(e), "file_id": file_id}
        await manager.broadcast(error_response, file_id) # Push error too
        # è®°å½•å¤±è´¥çŠ¶æ€åˆ°æ•°æ®åº“ï¼ˆå¯é€‰ï¼Œæˆ–è€…å‰ç«¯æŸ¥è¯¢ä¸åˆ°ç»“æœè§†ä¸ºå¤±è´¥/å¤„ç†ä¸­ï¼‰
        # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å°è¯•æ›´æ–°ä¸€ä¸ª"failed"è®°å½•ï¼Œæˆ–è€…ä¸åšå¤„ç†ï¼Œå‰ç«¯è¶…æ—¶
        try:
             fail_entry = AnalysisHistory(
                id=str(uuid.uuid4()),
                file_id=file_id,
                filename=filename,
                analysis_time=time.time()
            )
             fail_entry.result_dict = {"status": "failed", "error": str(e)}
             db.add(fail_entry)
             db.commit()
        except:
             pass
    finally:
        db.close()

@app.get("/analysis/{file_id}/status")
async def get_analysis_status(file_id: str):
    """æŸ¥è¯¢æ–‡ä»¶åˆ†æçŠ¶æ€"""
    db = SessionLocal()
    try:
        # æŸ¥è¯¢å†å²è®°å½•æ˜¯å¦å­˜åœ¨
        record = db.query(AnalysisHistory).filter(AnalysisHistory.file_id == file_id).order_by(AnalysisHistory.created_at.desc()).first()
        
        if not record:
            # å¯èƒ½æ˜¯æ­£åœ¨å¤„ç†ä¸­ï¼Œæˆ–è€…æ ¹æœ¬ä¸å­˜åœ¨
            # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾ä¸å­˜åœ¨å°±æ˜¯è¿˜æ²¡å¤„ç†å®Œï¼ˆå› ä¸ºæˆ‘ä»¬æ˜¯å…ˆè¿”å›å†å¤„ç†ï¼‰
            # æ›´å¥½çš„åšæ³•æ˜¯æœ‰ä¸€ä¸ªç‹¬ç«‹çš„ Task è¡¨è®°å½•çŠ¶æ€
            # è¿™é‡Œæš‚æ—¶è¿”å› processingï¼Œå‰ç«¯è®¾ç½®è¶…æ—¶
            return {"status": "processing"}
        
        result = record.result_dict
        if result.get("status") == "failed":
             return {"status": "failed", "error": result.get("error")}
        
        # å¦‚æœæœ‰ç»“æœï¼Œä¸”ä¸æ˜¯failedï¼Œå°±æ˜¯completed
        # å…¼å®¹æ—§æ•°æ®ï¼šå¦‚æœæ²¡æœ‰ status å­—æ®µä½†æœ‰ mindmapï¼Œä¹Ÿæ˜¯ completed
        if result.get("status") == "completed" or "mindmap" in result:
             # è¿”å›å®Œæ•´ç»“æœ
             result["status"] = "completed"
             return result
        
        return {"status": "processing"}
    finally:
        db.close()
    


@app.post("/upload-simple")
async def upload_file_simple(file: UploadFile = File(...)):
    """ç®€å•ä¸Šä¼ æ¥å£ï¼Œä»…ä¿å­˜æ–‡ä»¶åˆ°uploadsç›®å½•å¹¶è®°å½•åˆ°æ•°æ®åº“ï¼Œä¸è¿›è¡ŒAIåˆ†æ"""
    try:
        logger.info(f"ç®€å•ä¸Šä¼ æ–‡ä»¶: {file.filename}, ç±»å‹: {file.content_type}")
        file_id = str(uuid.uuid4())
        ext = file.filename.split('.')[-1].lower()
        file_path = UPLOAD_DIR / f"{file_id}.{ext}"
        
        # åªæ”¯æŒPDFæ–‡ä»¶
        if ext != "pdf":
            raise HTTPException(status_code=400, detail="ä»…æ”¯æŒPDFæ–‡ä»¶ä¸Šä¼ ")
        
        file_content = await file.read()
        with open(file_path, "wb") as f:
            f.write(file_content)
        logger.info(f"æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")
        
        # ä¿å­˜æ–‡ä»¶ä¿¡æ¯åˆ°æ•°æ®åº“
        save_file_to_db(file_id, file.filename)
        
        # è¿”å›ç®€å•çš„å“åº”
        response_data = {
            "file_id": file_id,
            "filename": file.filename
        }
        
        logger.info("ç®€å•æ–‡ä»¶ä¸Šä¼ å®Œæˆ")
        return Response(
            content=json.dumps(response_data, ensure_ascii=False),
            media_type="application/json"
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç®€å•æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}", exc_info=True)
        error_response = {
            "file_id": "",
            "filename": file.filename if 'file' in locals() else "",
            "error": str(e)
        }
        return Response(
            content=json.dumps(error_response, ensure_ascii=False),
            media_type="application/json",
            status_code=500
        )



@app.get("/files")
async def get_files(
    page: int = Query(1, ge=1, description="é¡µç ï¼Œä»1å¼€å§‹"),
    page_size: int = Query(10, ge=1, le=100, description="æ¯é¡µæ˜¾ç¤ºçš„æ–‡ä»¶æ•°é‡"),
    file_type: str = Query(None, description="å¯é€‰çš„æ–‡ä»¶ç±»å‹è¿‡æ»¤ï¼Œå¦‚'pdf'ã€'txt'ç­‰")
):
    """è·å–ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨ï¼Œæ”¯æŒåˆ†é¡µ"""
    try:
        files = []
        
        # ä½¿ç”¨æ•°æ®åº“ä¼šè¯è·å–æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯
        db = SessionLocal()
        try:
            # è·å–æ‰€æœ‰æ–‡ä»¶å­˜å‚¨è®°å½•
            file_records = db.query(FileTextStore).all()
            file_store_dict = {record.file_id: record for record in file_records}
        finally:
            db.close()
        
        # åªå¤„ç†åœ¨FileTextStoreä¸­æœ‰è®°å½•çš„æ–‡ä»¶
        for file_id, record in file_store_dict.items():
            # æ£€æŸ¥æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶ç±»å‹
            found = False
            # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶ç±»å‹è¿‡æ»¤ï¼Œåˆ™åªæ£€æŸ¥è¯¥ç±»å‹
            if file_type:
                # åªæ£€æŸ¥æŒ‡å®šçš„æ–‡ä»¶ç±»å‹
                ext = file_type
                file_path = os.path.join(str(UPLOAD_DIR), f"{file_id}.{ext}")
                if os.path.isfile(file_path):
                    # è·å–æ–‡ä»¶å¤§å°
                    file_size = os.path.getsize(file_path)
                    
                    # ä½¿ç”¨æ•°æ®åº“ä¸­çš„åŸå§‹æ–‡ä»¶åå’Œä¸Šä¼ æ—¶é—´
                    original_filename = record.original_filename or f"{file_id}.{ext}"
                    upload_time = record.upload_time or os.path.getmtime(file_path)
                    
                    # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœæ˜¯PDFç±»å‹ï¼Œç¡®ä¿æ–‡ä»¶ååŒ…å«.pdfæ‰©å±•å
                    if ext == 'pdf' and not original_filename.lower().endswith('.pdf'):
                        continue
                    
                    files.append({
                        "file_id": file_id,
                        "filename": original_filename,
                        "upload_time": upload_time,
                        "size": file_size,
                        "type": ext
                    })
                    found = True
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šæ–‡ä»¶ç±»å‹ï¼Œæ£€æŸ¥æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶ç±»å‹
                for ext in ["pdf", "txt", "log", "jpg", "jpeg", "png"]:
                    file_path = os.path.join(str(UPLOAD_DIR), f"{file_id}.{ext}")
                    if os.path.isfile(file_path):
                        # è·å–æ–‡ä»¶å¤§å°
                        file_size = os.path.getsize(file_path)
                        
                        # ä½¿ç”¨æ•°æ®åº“ä¸­çš„åŸå§‹æ–‡ä»¶åå’Œä¸Šä¼ æ—¶é—´
                        original_filename = record.original_filename or f"{file_id}.{ext}"
                        upload_time = record.upload_time or os.path.getmtime(file_path)
                        
                        files.append({
                            "file_id": file_id,
                            "filename": original_filename,
                            "upload_time": upload_time,
                            "size": file_size,
                            "type": ext
                        })
                        found = True
                        break  # åªæ·»åŠ ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ–‡ä»¶ï¼ˆé€šå¸¸åªæœ‰ä¸€ä¸ªæ‰©å±•åï¼‰
        
        # æŒ‰ä¸Šä¼ æ—¶é—´é™åºæ’åº
        files.sort(key=lambda x: x["upload_time"], reverse=True)
        
        # è®¡ç®—æ€»æ•°å’Œåˆ†é¡µä¿¡æ¯ - ç°åœ¨æ€»æ•°æ˜¯å®é™…æ˜¾ç¤ºçš„æ–‡ä»¶æ•°
        total = len(files)
        start = (page - 1) * page_size
        end = start + page_size
        paginated_files = files[start:end]
        
        return {
            "status": "success",
            "files": paginated_files,
            "pagination": {
                "total": total,
                "page": page,
                "page_size": page_size,
                "total_pages": (total + page_size - 1) // page_size
            }
        }
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.delete("/files/{file_id}")
async def delete_file(file_id: str, db: Session = Depends(get_db)):
    """åˆ é™¤å•ä¸ªæ–‡ä»¶ï¼ˆä¿ç•™åˆ†æå†å²è®°å½•ï¼‰"""
    try:
        # æ¸…ç†æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ–‡ä»¶
        deleted_files = []
        for ext in ["pdf", "txt", "log", "jpg", "jpeg", "png"]:
            file_path = UPLOAD_DIR / f"{file_id}.{ext}"
            if file_path.exists():
                try:
                    file_path.unlink()
                    deleted_files.append(str(file_path))
                    logger.info(f"å·²åˆ é™¤æ–‡ä»¶: {file_path}")
                except Exception as e:
                    logger.error(f"åˆ é™¤æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
        
        # æ¸…ç†å…¨å±€å­˜å‚¨ï¼ˆåªæ¸…ç†å‘é‡å­˜å‚¨ï¼ŒFILE_TEXT_STOREç°åœ¨ä»æ•°æ®åº“è·å–ï¼‰
        if file_id in VECTOR_STORES:
            del VECTOR_STORES[file_id]
            
        # [FIX] æ¸…ç†æ•°æ®åº“è®°å½•
        db.query(FileTextStore).filter(FileTextStore.file_id == file_id).delete()
        
        # [NEW] æ¸…ç†çŸ¥è¯†å›¾è°±
        try:
            from services.graph_service import GraphService
            GraphService(db).remove_document_knowledge(file_id)
        except Exception as ge:
            logger.error(f"æ¸…ç†å›¾è°±æ•°æ®å¤±è´¥: {ge}")
        
        db.commit()
        
        # ä¸å†æ¸…ç†ç›¸å…³çš„åˆ†æå†å²è®°å½•ï¼ˆæ ¹æ®éœ€æ±‚ä¿ç•™ï¼‰
        deleted_history_count = 0
        
        return {
            "status": "success",
            "message": f"åˆ é™¤æˆåŠŸï¼Œå…±åˆ é™¤ {len(deleted_files)} ä¸ªæ–‡ä»¶å’Œ {deleted_history_count} æ¡åˆ†æè®°å½•",
            "deleted_files": deleted_files,
            "deleted_history_count": deleted_history_count
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤æ“ä½œå¤±è´¥: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æ“ä½œå¤±è´¥: {str(e)}")

@app.delete("/files")
async def delete_all_files(db: Session = Depends(get_db)):
    """åˆ é™¤æ‰€æœ‰æ–‡ä»¶ï¼ˆä¿ç•™åˆ†æå†å²è®°å½•ï¼‰"""
    try:
        from services.graph_service import GraphService
        graph_service = GraphService(db)
        
        # æ‰«ææ–‡ä»¶ç³»ç»Ÿä¸­çš„æ‰€æœ‰æ–‡ä»¶
        deleted_count = 0
        processed_file_ids = set()
        
        # éå†æ‰€æœ‰å¯èƒ½çš„æ–‡ä»¶æ‰©å±•å
        for ext in ["pdf", "txt", "log", "jpg", "jpeg", "png"]:
            # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
            for file_path in UPLOAD_DIR.glob(f"*.{ext}"):
                if file_path.is_file():
                    # æå–æ–‡ä»¶IDï¼ˆæ–‡ä»¶åä¸åŒ…å«æ‰©å±•åï¼‰
                    file_id = file_path.stem
                    
                    # ç¡®ä¿æ¯ä¸ªæ–‡ä»¶IDåªå¤„ç†ä¸€æ¬¡
                    if file_id not in processed_file_ids:
                        processed_file_ids.add(file_id)
                        
                        # åˆ é™¤æ‰€æœ‰ä¸è¯¥æ–‡ä»¶IDç›¸å…³çš„æ–‡ä»¶ï¼ˆæ— è®ºæ‰©å±•åï¼‰
                        for del_ext in ["pdf", "txt", "log", "jpg", "jpeg", "png"]:
                            del_file_path = UPLOAD_DIR / f"{file_id}.{del_ext}"
                            if del_file_path.exists():
                                del_file_path.unlink()
                                deleted_count += 1
                                logger.info(f"å·²åˆ é™¤æ–‡ä»¶: {del_file_path}")
                        
                        # [FIX] æ¸…ç†æ•°æ®åº“è®°å½•
                        db.query(FileTextStore).filter(FileTextStore.file_id == file_id).delete()
                        
                        # [NEW] æ¸…ç†çŸ¥è¯†å›¾è°±
                        try:
                            graph_service.remove_document_knowledge(file_id)
                        except Exception as ge:
                            logger.error(f"æ¸…ç†å›¾è°±æ•°æ®å¤±è´¥: {ge}")

        # æ¸…ç†å…¨å±€å­˜å‚¨ï¼ˆåªæ¸…ç†å‘é‡å­˜å‚¨ï¼ŒFILE_TEXT_STOREç°åœ¨ä»æ•°æ®åº“è·å–ï¼‰
        for file_id in list(VECTOR_STORES.keys()):
            del VECTOR_STORES[file_id]
        
        db.commit()
        
        # ä¸å†æ¸…ç†åˆ†æå†å²è®°å½•ï¼ˆæ ¹æ®éœ€æ±‚ä¿ç•™ï¼‰
        deleted_history_count = 0
        
        return {
            "status": "success",
            "message": f"æ‰€æœ‰æ–‡ä»¶åˆ é™¤æˆåŠŸï¼Œå…±åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶",
            "deleted_count": deleted_count,
            "deleted_history_count": deleted_history_count
        }
    except Exception as e:
        logger.error(f"åˆ é™¤æ‰€æœ‰æ–‡ä»¶å¤±è´¥: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æ‰€æœ‰æ–‡ä»¶å¤±è´¥: {str(e)}")

@app.get("/history")
async def get_analysis_history(
    page: int = Query(1, ge=1, description="é¡µç ï¼Œä»1å¼€å§‹"), 
    page_size: int = Query(10, ge=1, le=100, description="æ¯é¡µæ˜¾ç¤ºçš„å†å²è®°å½•æ•°é‡"),
    db: Session = Depends(get_db)
):
    """è·å–åˆ†æå†å²è®°å½•ï¼Œæ”¯æŒåˆ†é¡µ"""
    try:
        # è®¡ç®—æ€»æ•°
        total = db.query(AnalysisHistory).count()
        
        # è®¡ç®—åç§»é‡å’Œé™åˆ¶
        offset = (page - 1) * page_size
        
        # æŸ¥è¯¢æ•°æ®ï¼ˆæŒ‰åˆ†ææ—¶é—´å€’åºæ’åˆ—ï¼‰
        history_entries = db.query(AnalysisHistory).order_by(
            AnalysisHistory.analysis_time.desc()
        ).offset(offset).limit(page_size).all()
        
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        paginated_history = []
        for entry in history_entries:
            paginated_history.append({
                "id": entry.id,
                "file_id": entry.file_id,
                "filename": entry.filename,
                "analysis_time": entry.analysis_time,
                "result": entry.result_dict
            })
        
        return {
            "status": "success",
            "total": total,
            "page": page,
            "page_size": page_size,
            "data": paginated_history
        }
    except Exception as e:
        logger.error(f"è·å–å†å²åˆ†æç»“æœå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å†å²åˆ†æç»“æœå¤±è´¥: {str(e)}")

@app.get("/history/{history_id}")
async def get_analysis_history_detail(
    history_id: str,
    db: Session = Depends(get_db)
):
    """è·å–æŒ‡å®šå†å²è®°å½•çš„è¯¦æƒ…"""
    try:
        # æŸ¥æ‰¾æŒ‡å®šçš„å†å²è®°å½•
        history_item = db.query(AnalysisHistory).filter(AnalysisHistory.id == history_id).first()
        if not history_item:
            raise HTTPException(status_code=404, detail="å†å²è®°å½•ä¸å­˜åœ¨")
        
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        result_item = {
            "id": history_item.id,
            "file_id": history_item.file_id,
            "filename": history_item.filename,
            "analysis_time": history_item.analysis_time,
            "result": history_item.result_dict
        }
        
        return {
            "status": "success",
            "data": result_item
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å†å²åˆ†æç»“æœè¯¦æƒ…å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å†å²åˆ†æç»“æœè¯¦æƒ…å¤±è´¥: {str(e)}")

@app.delete("/history/{history_id}")
async def delete_analysis_history(
    history_id: str,
    db: Session = Depends(get_db)
):
    """åˆ é™¤æŒ‡å®šå†å²åˆ†æè®°å½•ï¼Œå¹¶åŒæ—¶åˆ é™¤å¯¹åº”çš„æ–‡ä»¶"""
    try:
        # æŸ¥æ‰¾æŒ‡å®šçš„å†å²è®°å½•
        history_item = db.query(AnalysisHistory).filter(AnalysisHistory.id == history_id).first()
        if not history_item:
            raise HTTPException(status_code=404, detail="å†å²è®°å½•ä¸å­˜åœ¨")
        
        # è·å–å¯¹åº”çš„æ–‡ä»¶ID
        file_id = history_item.file_id
        
        # åˆ é™¤å†å²è®°å½•
        db.delete(history_item)
        deleted_history_count = 1
        logger.info(f"å·²åˆ é™¤å†å²è®°å½•: {history_id}")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å…¶ä»–å†å²è®°å½•ä½¿ç”¨åŒä¸€ä¸ªæ–‡ä»¶ID
        other_history_count = db.query(AnalysisHistory).filter(
            AnalysisHistory.file_id == file_id and AnalysisHistory.id != history_id
        ).count()
        
        # å¦‚æœæ²¡æœ‰å…¶ä»–å†å²è®°å½•ä½¿ç”¨åŒä¸€ä¸ªæ–‡ä»¶IDï¼Œåˆ™åˆ é™¤è¯¥æ–‡ä»¶
        deleted_files = []
        if other_history_count == 0:
            # æ¸…ç†æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ–‡ä»¶
            for ext in ["pdf", "txt", "log", "jpg", "jpeg", "png"]:
                file_path = UPLOAD_DIR / f"{file_id}.{ext}"
                if file_path.exists():
                    try:
                        file_path.unlink()
                        deleted_files.append(str(file_path))
                        logger.info(f"å·²åˆ é™¤æ–‡ä»¶: {file_path}")
                    except Exception as e:
                        logger.error(f"åˆ é™¤æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
            
            # æ¸…ç†æ•°æ®åº“ä¸­çš„æ–‡ä»¶è®°å½•
            db.query(FileTextStore).filter(FileTextStore.file_id == file_id).delete()
            
            # æ¸…ç†å‘é‡å­˜å‚¨
            if file_id in VECTOR_STORES:
                del VECTOR_STORES[file_id]
                
            # [NEW] æ¸…ç†çŸ¥è¯†å›¾è°±
            try:
                from services.graph_service import GraphService
                GraphService(db).remove_document_knowledge(file_id)
            except Exception as ge:
                logger.error(f"æ¸…ç†å›¾è°±æ•°æ®å¤±è´¥: {ge}")
        
        # æäº¤æ•°æ®åº“æ›´æ”¹
        db.commit()
        
        return {
            "status": "success",
            "message": f"åˆ é™¤æˆåŠŸï¼Œå…±åˆ é™¤ {deleted_history_count} æ¡åˆ†æè®°å½•å’Œ {len(deleted_files)} ä¸ªæ–‡ä»¶",
            "deleted_history_count": deleted_history_count,
            "deleted_files": deleted_files
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤å†å²åˆ†æè®°å½•å¤±è´¥: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"åˆ é™¤å†å²åˆ†æè®°å½•å¤±è´¥: {str(e)}")

@app.delete("/history")
async def delete_all_analysis_history(db: Session = Depends(get_db)):
    """åˆ é™¤æ‰€æœ‰å†å²åˆ†æè®°å½•ï¼Œå¹¶åŒæ—¶åˆ é™¤æ‰€æœ‰å¯¹åº”çš„æ–‡ä»¶"""
    try:
        # è·å–æ‰€æœ‰å”¯ä¸€çš„æ–‡ä»¶ID
        file_ids = db.query(AnalysisHistory.file_id).distinct().all()
        file_ids = [item[0] for item in file_ids]
        
        # åˆ é™¤æ‰€æœ‰å†å²è®°å½•
        deleted_history_count = db.query(AnalysisHistory).count()
        db.query(AnalysisHistory).delete()
        logger.info(f"å·²åˆ é™¤æ‰€æœ‰å†å²è®°å½•: {deleted_history_count} æ¡")
        
        # åˆ é™¤æ‰€æœ‰å¯¹åº”çš„æ–‡ä»¶
        deleted_files = []
        for file_id in file_ids:
            # æ¸…ç†æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ–‡ä»¶
            for ext in ["pdf", "txt", "log", "jpg", "jpeg", "png"]:
                file_path = UPLOAD_DIR / f"{file_id}.{ext}"
                if file_path.exists():
                    try:
                        file_path.unlink()
                        deleted_files.append(str(file_path))
                        logger.info(f"å·²åˆ é™¤æ–‡ä»¶: {file_path}")
                    except Exception as e:
                        logger.error(f"åˆ é™¤æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
            
            # æ¸…ç†æ•°æ®åº“ä¸­çš„æ–‡ä»¶è®°å½•
            db.query(FileTextStore).filter(FileTextStore.file_id == file_id).delete()
            
            # æ¸…ç†å‘é‡å­˜å‚¨
            if file_id in VECTOR_STORES:
                del VECTOR_STORES[file_id]
                
            # [NEW] æ¸…ç†çŸ¥è¯†å›¾è°±
            try:
                from services.graph_service import GraphService
                GraphService(db).remove_document_knowledge(file_id)
            except Exception as ge:
                logger.error(f"æ¸…ç†å›¾è°±æ•°æ®å¤±è´¥: {ge}")
        
        # æäº¤æ•°æ®åº“æ›´æ”¹
        db.commit()
        
        return {
            "status": "success",
            "message": f"åˆ é™¤æˆåŠŸï¼Œå…±åˆ é™¤ {deleted_history_count} æ¡åˆ†æè®°å½•å’Œ {len(deleted_files)} ä¸ªæ–‡ä»¶",
            "deleted_history_count": deleted_history_count,
            "deleted_files": deleted_files
        }
    except Exception as e:
        logger.error(f"åˆ é™¤æ‰€æœ‰å†å²åˆ†æè®°å½•å¤±è´¥: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æ‰€æœ‰å†å²åˆ†æè®°å½•å¤±è´¥: {str(e)}")

@app.get("/")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {
        "status": "success",
        "message": "DocMind Pro API is running"
    }

# ======================
# è§†é¢‘åˆ†æç›¸å…³APIè·¯ç”±
# ======================
# ======================
# Semantic Teacher Mode (User Memory)
# ======================
class UserMemory:
    def __init__(self):
        # Key: file_id, Value: List of memory dicts
        self.memories = {}

    def add_memory(self, file_id, question, correction):
        """Add a correction to semantic memory for a specific file"""
        if GLOBAL_HISTORY_STORE.model:
            vector = GLOBAL_HISTORY_STORE.model.encode(question)
            try:
                norm = np.linalg.norm(vector)
                if norm > 0:
                    vector = vector / norm
            except:
                pass
            
            if file_id not in self.memories:
                self.memories[file_id] = []
                
            file_memories = self.memories[file_id]
            
            # Check for existing similar memory (Threshold > 0.95)
            best_idx = -1
            best_sim = -1.0
            
            for idx, mem in enumerate(file_memories):
                try:
                    sim = np.dot(vector, mem["vector"])
                    if sim > best_sim:
                        best_sim = sim
                        best_idx = idx
                except:
                    continue
            
            # Merge if highly similar
            if best_idx >= 0 and best_sim > 0.95:
                current_text = file_memories[best_idx]["correction"]
                if correction not in current_text:
                    new_correction = current_text + "\n" + correction
                    file_memories[best_idx]["correction"] = new_correction
                    
                    # [DB] Update existing rule
                    try:
                        db = SessionLocal()
                        rule = db.query(TeacherRule).filter(TeacherRule.id == file_memories[best_idx]["id"]).first()
                        if rule:
                            rule.correction = new_correction
                            db.commit()
                        db.close()
                    except Exception as e:
                        logger.error(f"Failed to update rule in DB: {e}")

                    logger.info(f"ğŸ§  [Teacher Mode] MERGED concept (Sim: {best_sim:.2f}) for File {file_id}: {question} -> {new_correction}")
                else:
                    logger.info(f"ğŸ§  [Teacher Mode] SKIPPED duplicate concept (Sim: {best_sim:.2f})")
                
                file_memories[best_idx]["timestamp"] = time.time()
                return

            # [DB] Create new rule
            rule_id = str(uuid.uuid4())
            try:
                db = SessionLocal()
                new_rule = TeacherRule(
                    id=rule_id,
                    file_id=file_id,
                    question=question,
                    correction=correction
                )
                db.add(new_rule)
                db.commit()
                db.close()
            except Exception as e:
                logger.error(f"Failed to save rule to DB: {e}")

            file_memories.append({
                "id": rule_id,
                "vector": vector,
                "question": question,
                "correction": correction,
                "timestamp": time.time()
            })
            logger.info(f"ğŸ§  [Teacher Mode] Learned new concept for File {file_id}: {question} -> {correction}")
            
    # Optimized threshold for better precision
    def search_memory(self, file_id, question, threshold=0.62):
        """Search for relevant corrections within a specific file"""
        if file_id not in self.memories:
            logger.info(f"ğŸ§  [Memory Search] Skipped: No memories for File {file_id}")
            return None
            
        target_memories = self.memories[file_id]
        logger.info(f"ğŸ§  [Memory Search Entry] File: {file_id} | Query: '{question}' | Memories: {len(target_memories)}")
        
        if not GLOBAL_HISTORY_STORE.model:
            logger.error("ğŸ§  [Memory Search] Aborted: Embedding model not loaded!")
            return None
            
        q_vec = GLOBAL_HISTORY_STORE.model.encode(question)
        q_norm = np.linalg.norm(q_vec)
        if q_norm > 0:
            q_vec = q_vec / q_norm
            
        matches = []
        
        logger.info(f"ğŸ§  [Memory Search] Query: {question}")
        for mem in target_memories:
            score = np.dot(q_vec, mem["vector"])
            logger.info(f"   - Candidate: '{mem['question']}' | Score: {score:.4f}")
            if score >= threshold:
                matches.append({
                    "correction": mem["correction"],
                    "score": score
                })
        
        if matches:
            matches.sort(key=lambda x: x["score"], reverse=True)
            top_matches = matches[:3]
            
            seen_corrections = set()
            final_corrections = []
            for m in top_matches:
                parts = m["correction"].split('\n')
                for part in parts:
                    part = part.strip()
                    if part and part not in seen_corrections:
                        seen_corrections.add(part)
                        final_corrections.append(part)
            
            combined_correction = "\n".join(final_corrections)
            logger.info(f"ğŸ§  [Teacher Mode] Recall triggered (Top Score: {matches[0]['score']:.4f}). Combined {len(final_corrections)} facts.")
            return {"correction": combined_correction}
            
        logger.info(f"ğŸ§  [Teacher Mode] No match found")
        return None

GLOBAL_USER_MEMORY = UserMemory()

# [DEBUG] Endpoints for Teacher Mode
@app.get("/debug/memory")
async def get_debug_memory():
    """View current semantic memory"""
    all_items = []
    for fid, mems in GLOBAL_USER_MEMORY.memories.items():
        for m in mems:
            all_items.append({
                "file_id": fid,
                "question": m["question"],
                "correction": m["correction"],
                "timestamp": m["timestamp"]
            })
            
    return {
        "count": len(all_items),
        "items": all_items
    }

@app.post("/debug/check_memory")
async def check_debug_memory(payload: dict = Body(...)):
    """Test similarity search"""
    question = payload.get("question", "")
    target_file_id = payload.get("file_id", None)
    
    # Calculate all scores across all files
    scores = []
    if GLOBAL_HISTORY_STORE.model:
        q_vec = GLOBAL_HISTORY_STORE.model.encode(question)
        q_norm = np.linalg.norm(q_vec)
        if q_norm > 0: q_vec = q_vec / q_norm
        
        for fid, mems in GLOBAL_USER_MEMORY.memories.items():
            if target_file_id and fid != target_file_id:
                continue
                
            for m in mems:
                score = np.dot(q_vec, m["vector"])
                scores.append({
                    "file_id": fid,
                    "question": m["question"],
                    "correction": m["correction"],
                    "score": float(score)
                })
    
    match = None
    if target_file_id:
        match = GLOBAL_USER_MEMORY.search_memory(target_file_id, question, threshold=0.0)
    
    return {
        "best_match_for_file": match,
        "all_scores": sorted(scores, key=lambda x: x["score"], reverse=True)
    }

# [NEW] Persistence: Restore memory from DB on startup
def rebuild_memory_from_db():
    """Load from TeacherRule table. If empty, migrate from QAHistory (One-time)"""
    logger.info("ğŸ§  [Teacher Mode] Initializing: Restoring memory from database...")
    db = SessionLocal()
    count = 0
    try:
        # 1. Load from TeacherRule (Primary Source)
        rules = db.query(TeacherRule).all()
        if rules:
            logger.info(f"ğŸ§  [Teacher Mode] Loading {len(rules)} rules from TeacherRule table...")
            for r in rules:
                questions = r.question # Currently 1:1, but could be N:1
                # If question is stored as a list in string or just string.
                # Just add directly.
                # Note: We must inject the ID so we can update it later.
                if r.file_id and r.question and r.correction:
                    if r.file_id not in GLOBAL_USER_MEMORY.memories:
                        GLOBAL_USER_MEMORY.memories[r.file_id] = []
                    
                    if GLOBAL_HISTORY_STORE.model:
                        vector = GLOBAL_HISTORY_STORE.model.encode(r.question)
                        try:
                            norm = np.linalg.norm(vector)
                            if norm > 0: vector = vector / norm
                        except:
                            pass
                        
                        GLOBAL_USER_MEMORY.memories[r.file_id].append({
                            "id": r.id,
                            "vector": vector,
                            "question": r.question,
                            "correction": r.correction,
                            "timestamp": r.created_at.timestamp() if r.created_at else time.time()
                        })
                        count += 1
            logger.info(f"ğŸ§  [Teacher Mode] Loaded {count} rules from TeacherRule table.")
            return

        # 2. Migration: If TeacherRule is empty, scan QAHistory (Backward Compatibility)
        logger.warning("ğŸ§  [Teacher Mode] No rules found in TeacherRule. Scanning QAHistory for migration...")
        items = db.query(QAHistory).filter(QAHistory.evaluation.isnot(None)).all()
        
        for item in items:
            try:
                eval_data = json.loads(item.evaluation)
                if eval_data.get('rating', 0) < 0 and eval_data.get('comment'):
                    comment = eval_data.get('comment')
                    if len(comment.strip()) > 2:
                        question = item.question
                        # Use add_memory which handles DB insertion now!
                        GLOBAL_USER_MEMORY.add_memory(item.file_id, question, comment)
                        count += 1
            except Exception as e:
                logger.warning(f"Failed to parse evaluation for QA {item.id}: {e}")
                
        logger.info(f"ğŸ§  [Teacher Mode] Migration Complete. Migrated {count} memories to TeacherRule table.")
    except Exception as e:
        logger.error(f"ğŸ§  [Teacher Mode] Restoration Failed: {e}")
    finally:
        db.close()

# Execute restoration immediately on startup
rebuild_memory_from_db()

class FeedbackRequest(BaseModel):
    file_id: str
    qa_id: str
    rating: int
    comment: Optional[str] = None
    question: Optional[str] = None

@app.post("/feedback")
async def submit_feedback(feedback: FeedbackRequest):
    """Submit feedback and trigger Teacher Mode if applicable"""
    db = SessionLocal()
    try:
        # 1. Update DB (QAHistory)
        if feedback.qa_id:
             qa_item = db.query(QAHistory).filter(QAHistory.id == feedback.qa_id).first()
             if qa_item:
                 qa_item.evaluation = json.dumps({
                     "rating": feedback.rating,
                     "comment": feedback.comment
                 }, ensure_ascii=False)
                 db.commit()
            
        # 2. Teacher Mode Logic (Semantic Learning)
        # Condition: Rating is Negative (-1) AND Comment (Correction) is provided
        if feedback.rating < 0 and feedback.comment and len(feedback.comment.strip()) > 2:
            question_text = feedback.question
            if not question_text and qa_item:
                question_text = qa_item.question
            
            if question_text:
                # Store in Semantic Memory
                GLOBAL_USER_MEMORY.add_memory(feedback.file_id, question_text, feedback.comment)
                return {"status": "success", "message": "å·²å­¦ä¹ æ–°çŸ¥è¯† (Teacher Mode Active)"}
                
        return {"status": "success", "message": "Feedback received"}
    except Exception as e:
        logger.error(f"Feedback error: {e}")
        return {"status": "error", "error": str(e)}
    finally:
        db.close()



# ======================
# çŸ¥è¯†åº“ç®¡ç†API
# ======================
@app.get("/api/rules")
async def get_teacher_rules(file_id: Optional[str] = None):
    """Get all teacher rules, optionally filtered by file"""
    db = SessionLocal()
    try:
        query = db.query(TeacherRule)
        if file_id:
            query = query.filter(TeacherRule.file_id == file_id)
        
        rules = query.order_by(TeacherRule.created_at.desc()).all()
        return {
            "count": len(rules),
            "items": [
                {
                    "id": r.id,
                    "file_id": r.file_id,
                    "question": r.question,
                    "correction": r.correction,
                    "created_at": r.created_at.isoformat() if r.created_at else None
                } for r in rules
            ]
        }
    finally:
        db.close()

@app.delete("/api/rules/{rule_id}")
async def delete_teacher_rule(rule_id: str):
    """Delete a teacher rule"""
    db = SessionLocal()
    try:
        rule = db.query(TeacherRule).filter(TeacherRule.id == rule_id).first()
        if not rule:
            raise HTTPException(status_code=404, detail="Rule not found")
        
        # 1. Delete from DB
        file_id = rule.file_id
        db.delete(rule)
        db.commit()
        
        # 2. Update In-Memory cache
        if file_id in GLOBAL_USER_MEMORY.memories:
            original_len = len(GLOBAL_USER_MEMORY.memories[file_id])
            GLOBAL_USER_MEMORY.memories[file_id] = [
                m for m in GLOBAL_USER_MEMORY.memories[file_id] 
                if m.get("id") != rule_id
            ]
            new_len = len(GLOBAL_USER_MEMORY.memories[file_id])
            logger.info(f"ğŸ§  [Teacher Mode] Deleted rule {rule_id} from memory. ({original_len} -> {new_len})")
            
        return {"status": "success", "message": "Rule deleted"}
    finally:
        db.close()

# ======================
# è§†é¢‘åˆ†æç›¸å…³APIè·¯ç”±
# ======================
@app.get("/ask")
async def ask_question(
    question: str = Query(..., description="ç”¨æˆ·æé—®"),
    file_id: str = Query(..., description="æ–‡ä»¶IDï¼Œæ¥è‡ª /upload è¿”å›")
):
    # ä½¿ç”¨æ•°æ®åº“ä¼šè¯è·å–æ–‡ä»¶ä¿¡æ¯
    db = SessionLocal()
    try:
        # [TEACHER MODE POINTER]
        # Using a higher threshold (0.62) to avoid irrelevant recollections
        logger.info(f"âš¡ [Debug] Checking UserMemory for: '{question}' (File: {file_id})")
        teacher_instruction = ""  # Default empty string if no match
        memory_match = GLOBAL_USER_MEMORY.search_memory(file_id, question, threshold=0.62)
        if memory_match:
            # Simplified / Softer Prompt
            teacher_instruction = (
                f"\n\n[User Correction / ç”¨æˆ·æŒ‡æ­£]\n"
                f"Note: The user previously corrected similar concepts: '{memory_match['correction']}'\n"
                f"Instruction: IF this correction is directly relevant to the current question, use it as the ground truth. "
                f"Otherwise, ignore it."
            )
            logger.info(f"ğŸ’¡ Injecting Teacher Instruction: {teacher_instruction}")

        # ä»æ•°æ®åº“ä¸­è·å–æ–‡ä»¶è®°å½•
        file_record = db.query(FileTextStore).filter(FileTextStore.file_id == file_id).first()
        if not file_record:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·å…ˆä¸Šä¼ ")
        
        if not question.strip():
            raise HTTPException(status_code=400, detail="é—®é¢˜ä¸èƒ½ä¸ºç©º")
        
        # [NEW] é˜²é‡å¤æäº¤é€»è¾‘
        import datetime
        five_seconds_ago = datetime.datetime.now() - datetime.timedelta(seconds=5)
        
        existing_qa = db.query(QAHistory).filter(
            QAHistory.file_id == file_id,
            QAHistory.question == question,
            QAHistory.created_at >= five_seconds_ago
        ).order_by(QAHistory.created_at.desc()).first()
        
        if existing_qa:
            logger.warning(f"æ£€æµ‹åˆ°é‡å¤è¯·æ±‚ (5ç§’å†…): File={file_id}, Q={question}")
            try:
                evidence_data = json.loads(existing_qa.evidence_list)
            except:
                evidence_data = []
            return {
                "qa_id": existing_qa.id,
                "answer": existing_qa.answer,
                "evidence": evidence_data,
                "note": "cached"
            }
 
        lower_q = question.lower()
        if any(trigger in lower_q for trigger in ["å…³é”®å­—", "å…³é”®è¯", "keyword", "keywords"]):
            keywords = file_record.keywords_list
            if keywords:
                return {"answer": ", ".join(keywords)}
            else:
                return {"answer": "æ–‡æ¡£ä¸­æœªæåŠæ­¤å†…å®¹ã€‚"}
        
        vs = VECTOR_STORES.get(file_id)
        if vs is None:
            chunks = file_record.chunks_list
            relevant_chunks = chunks[:3]
        else:
            relevant_chunks = vs.search(question, k=3)
        
        logger.info(f"æ£€ç´¢åˆ° {len(relevant_chunks)} ä¸ªç›¸å…³ç‰‡æ®µ")
        
        # [TEACHER MODE INJECTION]
        final_question = question + teacher_instruction
        
        answer = await rag_answer(final_question, relevant_chunks)
        evidence = [{"text": c, "page": 1} for c in relevant_chunks]

        # æŒä¹…åŒ–ä¿å­˜ Q&A è®°å½•
        new_qa = QAHistory(
            id=str(uuid.uuid4()),
            file_id=file_id,
            question=question,
            answer=answer,
            evidence=json.dumps(evidence, ensure_ascii=False),
            evaluation=None # æ­¤æ—¶å°šæœªè¯„ä¼°
        )
        db.add(new_qa)
        db.commit()
        
        return {
            "qa_id": new_qa.id, # è¿”å›IDä¾›åç»­è¯„ä¼°æ›´æ–°ä½¿ç”¨
            "answer": answer, 
            "evidence": evidence
        }
    finally:
        db.close()


# ======================
# è§†é¢‘åˆ†æç›¸å…³APIè·¯ç”±
# ======================
@app.post("/api/process-video")
async def process_video(video_id: str = Body(..., description="è§†é¢‘ID"), 
                        task_type: str = Body(..., description="ä»»åŠ¡ç±»å‹"),
                        question: str = Body(None, description="é—®é¢˜ï¼ˆä»…QAä»»åŠ¡éœ€è¦ï¼‰")):
    """å¤„ç†è§†é¢‘ç›¸å…³ä»»åŠ¡çš„APIç«¯ç‚¹"""
    try:
        
        # QAä»»åŠ¡éœ€è¦é—®é¢˜å‚æ•°
        if task_type == 'qa' and not question:
            return Response(
                content=json.dumps({"error": "QAä»»åŠ¡éœ€è¦æä¾›questionå‚æ•°", "status": "error"}),
                media_type="application/json",
                status_code=400
            )
        
        logger.info(f"Received request: video_id={video_id}, task_type={task_type}")
        
        # æ£€æŸ¥è§†é¢‘å¤„ç†å™¨æ˜¯å¦å¯ç”¨
        if video_processor is None:
            return Response(
                content=json.dumps({"error": "è§†é¢‘å¤„ç†æœåŠ¡ä¸å¯ç”¨", "status": "error"}),
                media_type="application/json",
                status_code=500
            )
        
        # å¤„ç†è§†é¢‘ä»»åŠ¡
        result = video_processor.process_video_task(video_id, task_type, question)
        
        # è¿”å›æˆåŠŸå“åº”
        return Response(
            content=json.dumps({
                "success": True,
                "message": "å¤„ç†æˆåŠŸ",
                "data": result.get('data', {}),
                "task_type": task_type
            }),
            media_type="application/json",
            status_code=200
        )
        
    except ValueError as ve:
        # å¤„ç†å‚æ•°é”™è¯¯ç­‰é¢„æœŸå†…çš„é”™è¯¯
        logger.warning(f"Value error: {str(ve)}")
        return Response(
            content=json.dumps({"error": str(ve), "status": "error"}),
            media_type="application/json",
            status_code=400
        )
    
    except Exception as e:
        # å¤„ç†æœªé¢„æœŸçš„é”™è¯¯
        logger.error(f"Unexpected error: {str(e)}")
        return Response(
            content=json.dumps({"error": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}", "status": "error"}),
            media_type="application/json",
            status_code=500
        )

@app.get("/api/health")
async def video_health_check():
    """è§†é¢‘å¤„ç†æœåŠ¡å¥åº·æ£€æŸ¥APIç«¯ç‚¹"""
    return {
        "status": "healthy",
        "message": "Video processing service is running"
    }

# ======================
# PDFåŠ©æ‰‹ç›¸å…³APIè·¯ç”±
# ======================
@app.post("/api/pdf/extract-images")
async def extract_pdf_images(request: dict = Body(...)):
    file_id = request.get("file_id")
    """ä»PDFæ–‡ä»¶ä¸­æå–å›¾ç‰‡"""
    try:
        db = SessionLocal()
        file_record = db.query(FileTextStore).filter(FileTextStore.file_id == file_id).first()
        db.close()
        if not file_record:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        pdf_path = None
        for ext in ["pdf"]:
            path = UPLOAD_DIR / f"{file_id}.{ext}"
            if path.exists():
                pdf_path = str(path)
                break
        if not pdf_path:
            raise HTTPException(status_code=404, detail="PDFæ–‡ä»¶ä¸å­˜åœ¨")
        result = pdf_service.extract_images(pdf_path)
        return {"success": True, "message": "å›¾ç‰‡æå–æˆåŠŸ", "data": result}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æå–PDFå›¾ç‰‡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æå–PDFå›¾ç‰‡å¤±è´¥: {str(e)}")

@app.post("/api/pdf/compress")
async def compress_pdf_file(request: dict = Body(...)):
    file_id = request.get("file_id")
    """å‹ç¼©PDFæ–‡ä»¶"""
    try:
        db = SessionLocal()
        file_record = db.query(FileTextStore).filter(FileTextStore.file_id == file_id).first()
        db.close()
        if not file_record:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        pdf_path = None
        for ext in ["pdf"]:
            path = UPLOAD_DIR / f"{file_id}.{ext}"
            if path.exists():
                pdf_path = str(path)
                break
        if not pdf_path:
            raise HTTPException(status_code=404, detail="PDFæ–‡ä»¶ä¸å­˜åœ¨")
        output_path = str(DOWNLOAD_DIR / f"{file_id}_compressed.pdf")
        result = pdf_service.compress_pdf(pdf_path, output_path)
        return {"success": True, "message": "PDFå‹ç¼©æˆåŠŸ", "data": result}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å‹ç¼©PDFå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å‹ç¼©PDFå¤±è´¥: {str(e)}")

@app.post("/api/pdf/extract-text")
async def extract_pdf_text(request: dict = Body(...)):
    file_id = request.get("file_id")
    """ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    try:
        db = SessionLocal()
        file_record = db.query(FileTextStore).filter(FileTextStore.file_id == file_id).first()
        db.close()
        if not file_record:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        pdf_path = None
        for ext in ["pdf"]:
            path = UPLOAD_DIR / f"{file_id}.{ext}"
            if path.exists():
                pdf_path = str(path)
                break
        if not pdf_path:
            raise HTTPException(status_code=404, detail="PDFæ–‡ä»¶ä¸å­˜åœ¨")
        result = pdf_service.extract_text(pdf_path)
        return {"success": True, "message": "æ–‡æœ¬æå–æˆåŠŸ", "data": {"text": result}}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æå–PDFæ–‡æœ¬å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æå–PDFæ–‡æœ¬å¤±è´¥: {str(e)}")

@app.post("/api/pdf/rotate")
async def rotate_pdf_pages(request: dict = Body(...)):
    file_id = request.get("file_id")
    angle = request.get("angle")
    pages = request.get("pages", ":")
    """æ—‹è½¬PDFé¡µé¢"""
    try:
        db = SessionLocal()
        file_record = db.query(FileTextStore).filter(FileTextStore.file_id == file_id).first()
        db.close()
        if not file_record:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        pdf_path = None
        for ext in ["pdf"]:
            path = UPLOAD_DIR / f"{file_id}.{ext}"
            if path.exists():
                pdf_path = str(path)
                break
        if not pdf_path:
            raise HTTPException(status_code=404, detail="PDFæ–‡ä»¶ä¸å­˜åœ¨")
        output_path = str(DOWNLOAD_DIR / f"{file_id}_rotated.pdf")
        result = pdf_service.rotate_pdf(pdf_path, output_path, angle, pages)
        return {"success": True, "message": "PDFé¡µé¢æ—‹è½¬æˆåŠŸ", "data": result}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ—‹è½¬PDFé¡µé¢å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ—‹è½¬PDFé¡µé¢å¤±è´¥: {str(e)}")

@app.post("/api/pdf/split")
async def split_pdf_file(request: dict = Body(...)):
    file_id = request.get("file_id")
    split_page = request.get("split_page")
    pages_per_file = request.get("pages_per_file")
    """æ‹†åˆ†PDFæ–‡ä»¶"""
    try:
        db = SessionLocal()
        file_record = db.query(FileTextStore).filter(FileTextStore.file_id == file_id).first()
        db.close()
        if not file_record:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        pdf_path = None
        for ext in ["pdf"]:
            path = UPLOAD_DIR / f"{file_id}.{ext}"
            if path.exists():
                pdf_path = str(path)
                break
        if not pdf_path:
            raise HTTPException(status_code=404, detail="PDFæ–‡ä»¶ä¸å­˜åœ¨")
        output_dir = str(DOWNLOAD_DIR / f"{file_id}_split")
        result = pdf_service.split_pdf(pdf_path, output_dir, split_page, pages_per_file)
        # æå–æ‹†åˆ†åçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        split_files = [item['path'] for item in result]
        return {"success": True, "message": "PDFæ‹†åˆ†æˆåŠŸ", "data": {"split_files": split_files}}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ‹†åˆ†PDFå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ‹†åˆ†PDFå¤±è´¥: {str(e)}")

@app.post("/api/pdf/merge")
async def merge_pdf_files(request: dict = Body(...)):
    """åˆå¹¶PDFæ–‡ä»¶"""
    try:
        file_ids = request.get("file_ids", [])
        if not isinstance(file_ids, list) or len(file_ids) < 2:
            raise HTTPException(status_code=400, detail="è‡³å°‘éœ€è¦2ä¸ªPDFæ–‡ä»¶æ‰èƒ½åˆå¹¶")
        
        pdf_paths = []
        for file_id in file_ids:
            db = SessionLocal()
            file_record = db.query(FileTextStore).filter(FileTextStore.file_id == file_id).first()
            db.close()
            if not file_record:
                raise HTTPException(status_code=404, detail=f"æ–‡ä»¶ID {file_id} ä¸å­˜åœ¨")
            
            pdf_path = None
            for ext in ["pdf"]:
                path = UPLOAD_DIR / f"{file_id}.{ext}"
                if path.exists():
                    pdf_path = str(path)
                    break
            
            if not pdf_path:
                raise HTTPException(status_code=404, detail=f"PDFæ–‡ä»¶ {file_id} ä¸å­˜åœ¨")
            
            pdf_paths.append(pdf_path)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_file_id = str(uuid.uuid4())
        output_path = str(DOWNLOAD_DIR / f"{output_file_id}.pdf")
        
        result = pdf_service.merge_pdfs(pdf_paths, output_path)
        # è½¬æ¢ç»“æœæ ¼å¼ï¼Œå°†output_pathæ”¹ä¸ºmerged_pathä»¥é€‚åº”å‰ç«¯æœŸæœ›
        result_with_merged_path = {
            "merged_path": result["output_path"],
            "merged_files": result["merged_files"],
            "total_pages": result["total_pages"]
        }
        return {"success": True, "message": "PDFåˆå¹¶æˆåŠŸ", "data": result_with_merged_path}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆå¹¶PDFå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆå¹¶PDFå¤±è´¥: {str(e)}")

@app.get("/api/pdf/metadata/{file_id}")
async def get_pdf_metadata(file_id: str):
    """è·å–PDFæ–‡ä»¶å…ƒæ•°æ®"""
    try:
        db = SessionLocal()
        file_record = db.query(FileTextStore).filter(FileTextStore.file_id == file_id).first()
        db.close()
        if not file_record:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        pdf_path = None
        for ext in ["pdf"]:
            path = UPLOAD_DIR / f"{file_id}.{ext}"
            if path.exists():
                pdf_path = str(path)
                break
        if not pdf_path:
            raise HTTPException(status_code=404, detail="PDFæ–‡ä»¶ä¸å­˜åœ¨")
        result = pdf_service.get_metadata(pdf_path)
        return {"success": True, "message": "è·å–PDFå…ƒæ•°æ®æˆåŠŸ", "data": result}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–PDFå…ƒæ•°æ®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–PDFå…ƒæ•°æ®å¤±è´¥: {str(e)}")

        
        # æå–æ–‡æœ¬
        result = pdf_service.extract_text(pdf_path)
        
        return {
            "success": True,
            "message": "PDFæ–‡æœ¬æå–æˆåŠŸ",
            "data": {"text": result}
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æå–PDFæ–‡æœ¬å¤±è´¥: {str(e)}")
# ======================
# ç¿»è¯‘ç›¸å…³å®šä¹‰
# ======================
class TranslateRequest(BaseModel):
    text: str
    type: str = "translate" # é»˜è®¤æ˜¯ç¿»è¯‘ï¼Œå¯é€‰ "polish"
    context: Optional[str] = None # é¢„ç•™å­—æ®µ

class BatchTranslateRequest(BaseModel):
    items: List[dict] # {index, text}
    type: str = "translate"  # translate | polish

class BatchTranslateResponse(BaseModel):
    status: str
    translations: List[dict]  # {index, translation}
    message: Optional[str] = None

class ParagraphItem(BaseModel):
    index: int
    text: str

class ParagraphResponse(BaseModel):
    status: str = "success"
    file_id: str
    filename: str = ""
    paragraphs: List[ParagraphItem]

# ======================
# åˆ’è¯ç¿»è¯‘ç›¸å…³API
# ======================
@app.post("/api/translate")
async def translate_selection(req: TranslateRequest):
    """
    æ¥æ”¶å‰ç«¯é€‰ä¸­çš„æ–‡æœ¬ï¼Œè°ƒç”¨ LLM è¿›è¡Œå­¦æœ¯ç¿»è¯‘
    """
    try:
        if not req.text or len(req.text.strip()) == 0:
            return {"status": "error", "message": "ç¿»è¯‘å†…å®¹ä¸èƒ½ä¸ºç©º"}
        
        # é™åˆ¶é•¿åº¦é˜²æ­¢æ»¥ç”¨
        if len(req.text) > 2000:
             return {"status": "error", "message": "é€‰ä¸­æ–‡æœ¬è¿‡é•¿ï¼Œè¯·åˆ†æ®µ"}
        
        if req.type == "polish":
            result = await simple_llm.polish_academic_text(req.text)
        else:
            result = await simple_llm.translate_academic_text(req.text)
            

        logger.info(f"æ”¶åˆ°ç¿»è¯‘è¯·æ±‚ï¼Œé•¿åº¦: {len(req.text)}")
         
        return {
            "status": "success", 
            "original": req.text,
            "translation": result
        }
    except Exception as e:
        logger.error(f"API ERROR: {str(e)}")
        raise HTTPException(status_code=500, detail=f"API ERROR: {str(e)}")

# ======================
# åŒè¯­å¯¹ç…§-å…¨æ–‡æ®µè½è·å–
# ======================
import re
from typing import List

def _normalize_pdf_text(text: str) -> str:
    if not text:
        return ""

    # ç»Ÿä¸€æ¢è¡Œ
    text = text.replace("\r\n", "\n").replace("\r", "\n")

    # ä¿®å¤ PDF æ–­è¯ï¼š "pa-\nper" -> "paper"
    # åªåœ¨è¿å­—ç¬¦åé¢ç´§è·Ÿæ¢è¡Œä¸”ä¸‹ä¸€è¡Œæ˜¯å­—æ¯æ—¶åˆå¹¶ï¼Œé¿å…è¯¯ä¼¤å…¬å¼/åˆ—è¡¨
    text = re.sub(r"(\w)-\n(?=\w)", r"\1", text)

    # æŠŠ"è¡Œå†…æ¢è¡Œ"å˜ç©ºæ ¼ï¼ˆä¿ç•™ç©ºè¡Œä½œä¸ºæ®µè½è¾¹ç•Œï¼‰
    # å…ˆæŠŠç©ºè¡Œæ ‡è®°å‡ºæ¥
    text = re.sub(r"\n\s*\n+", "\n\n", text)  # å¤šç©ºè¡Œæ”¶æ•›
    blocks = text.split("\n\n")
    blocks = [re.sub(r"[ \t]+", " ", b.replace("\n", " ")).strip() for b in blocks]
    text = "\n\n".join([b for b in blocks if b])

    # æ ‡ç‚¹åç¼ºç©ºæ ¼ï¼ˆè‹±æ–‡é€—å·/åˆ†å·/å†’å·ï¼‰
    # ä½†ä¸æ”¹ URL/email
    def _safe_punct_space(s: str) -> str:
        masks = []
        def mask(m):
            masks.append(m.group(0))
            return f"__MASK_{len(masks)-1}__"

        s = re.sub(r"https?://\S+|www\.\S+|[\w\.-]+@[\w\.-]+\.\w+", mask, s)
        s = re.sub(r"([,;:])(?=\S)", r"\1 ", s)
        for i, v in enumerate(masks):
            s = s.replace(f"__MASK_{i}__", v)
        return s

    text = "\n\n".join(_safe_punct_space(b) for b in text.split("\n\n"))

    return text.strip()

def _split_to_sentences(text: str) -> List[str]:
    """
    å°½é‡æŒ‰å¥å­è¾¹ç•Œåˆ‡åˆ†ï¼ˆä¸­è‹±æ–‡æ ‡ç‚¹ï¼‰ã€‚
    å¦‚æœåˆ‡ä¸åŠ¨ï¼Œfallback ä¸ºæŒ‰ç©ºæ ¼ç²—åˆ‡ã€‚
    """
    if not text:
        return []

    # ä¿ç•™åˆ†éš”ç¬¦ï¼šæŠŠå¥æœ«æ ‡ç‚¹ä½œä¸ºå¥å­çš„ä¸€éƒ¨åˆ†
    parts = re.split(r"([ã€‚ï¼ï¼Ÿ.!?]+)\s*", text)
    sents = []
    buf = ""
    for i in range(0, len(parts), 2):
        seg = parts[i].strip()
        punct = parts[i + 1] if i + 1 < len(parts) else ""
        if not seg and not punct:
            continue
        s = (seg + punct).strip()
        if s:
            sents.append(s)

    if len(sents) >= 2:
        return sents

    # fallbackï¼šæŒ‰ç©ºæ ¼ç²—åˆ‡ï¼ˆé¿å…è¶…é•¿ä¸€æ•´æ®µï¼‰
    words = text.split(" ")
    out = []
    tmp = []
    for w in words:
        if not w:
            continue
        tmp.append(w)
        if len(" ".join(tmp)) >= 120:
            out.append(" ".join(tmp))
            tmp = []
    if tmp:
        out.append(" ".join(tmp))
    return out

import re
from typing import List

def format_paragraph_for_reading(
    text: str,
    *,
    break_on_numbering: bool = True,
    break_on_sentence: bool = True
) -> str:
    if not text:
        return ""

    t = text.strip().replace("\r\n", "\n").replace("\r", "\n")

    t = re.sub(
        r"(?m)^\s*(\d+(?:\.\d+)*)\s*\.?\s*\n+\s*([A-Z][A-Z0-9 \-]{2,})\b",
        r"\1. \2",
        t
    )

    t = re.sub(r"[ \t]+", " ", t)

    t = re.sub(
        r"(?m)^(?P<h>(?:\d+(?:\.\d+)*\.\s*)?[A-Z][A-Z0-9 \-]{2,})\s+(?P<body>[A-Z][a-z].+)$",
        r"\g<h>\n\g<body>",
        t
    )

    if break_on_numbering:
        t = re.sub(r"(?<!\n)\s+(?=(\d+(?:\.\d+)*)(?:\.)\s+[A-Z])", "\n", t)
        t = re.sub(r"(?<!\n)\s+(?=\d+(?:\.\d+)*\)\s+)", "\n", t)
        t = re.sub(r"(?<!\n)\s+(?=(\d+(?:\.\d+)*)(?::)\s+[A-Z])", "\n", t)

    if break_on_sentence:
        t = re.sub(r"(?<!\b\d)([.!?])\s+(?=[A-Z])", r"\1\n", t)
        t = re.sub(r"([ã€‚ï¼ï¼Ÿ])(?=[^\n])", r"\1\n", t)
        t = re.sub(r"(?<!\n)\s+(?=\[\d+\]\s+)", "\n", t)

    t = re.sub(r"\n{3,}", "\n\n", t)
    t = re.sub(r"[ \t]+\n", "\n", t)

    return t.strip()

def split_paragraphs_by_chars(
    text: str,
    target: int = 900,   # ç†æƒ³æ®µè½é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰
    min_len: int = 350,  # å°äºè¿™ä¸ªå°½é‡åˆå¹¶
) -> List[str]:
    """
    å…ˆæŠŠ PDF æ–‡æœ¬è§„æ•´æˆ"è‡ªç„¶æ®µ"ï¼ˆä¿ç•™åŒæ¢è¡Œï¼‰ï¼Œ
    å†æŠŠè‡ªç„¶æ®µæŒ‰å­—ç¬¦é•¿åº¦åˆ‡æˆé€‚åˆç¿»è¯‘çš„ä¸€æ®µæ®µï¼ˆå°½é‡æŒ‰å¥å­æ–­å¼€ï¼‰ã€‚
    """
    text = _normalize_pdf_text(text)
    if not text:
        return []

    # Step A: å…ˆæŒ‰ç©ºè¡Œåš"è‡ªç„¶æ®µ"
    raw_parts = [p.strip() for p in text.split("\n\n") if p.strip()]

    # Step B: åœ¨æ¯ä¸ªè‡ªç„¶æ®µé‡Œï¼Œå†æŒ‰"æ ‡é¢˜/ç« èŠ‚è¡Œ"åˆ‡ä¸€ä¸‹ï¼ˆæ›´åƒè®ºæ–‡ç»“æ„ï¼‰
    # æ ‡é¢˜/ç« èŠ‚å¸¸è§æ¨¡å¼ï¼šå…¨å¤§å†™ã€æ•°å­—ç¼–å·ã€Abstract/Keywords/References ç­‰
    heading_pat = re.compile(
        r"(?im)^(abstract|keywords?|references|bibliography|introduction|conclusion|acknowledg(e)?ments?)\b|^\d+(\.\d+)*\s+\S+|^[A-Z][A-Z0-9 \-]{6,}$"
    )

    parts = []
    for block in raw_parts:
        # å¦‚æœ block å†…å«å¤šä¸ª"heading"ï¼ŒæŒ‰ heading ä½ç½®å†æ‹†
        lines = block.split(" ")
        # ç”±äºæˆ‘ä»¬æŠŠè¡Œéƒ½åˆå¹¶äº†ï¼Œè¿™é‡Œç”¨"ä¼ªè¡Œ"ç­–ç•¥ï¼šåœ¨ heading å…³é”®è¯é™„è¿‘æ’æ–­ç‚¹
        # ç”¨æ­£åˆ™åœ¨æ–‡æœ¬ä¸­æ‰¾ heading ä½ç½®
        cuts = []
        for m in re.finditer(r"(?im)\b(ABSTRACT|KEYWORDS|REFERENCES|BIBLIOGRAPHY|INTRODUCTION|CONCLUSION)\b", block):
            if m.start() > 0:
                cuts.append(m.start())

        if not cuts:
            parts.append(block)
            continue

        cuts = sorted(set(cuts))
        last = 0
        for c in cuts:
            seg = block[last:c].strip()
            if seg:
                parts.append(seg)
            last = c
        tail = block[last:].strip()
        if tail:
            parts.append(tail)

    # Step C: å¯¹è¿‡é•¿æ®µè½ï¼Œä¼˜å…ˆåœ¨å¥æœ«é™„è¿‘åˆ‡æˆ target_len å·¦å³
    def split_by_sentence(s: str) -> List[str]:
        if len(s) <= target:
            return [s]

        # å…ˆæŒ‰å¼ºå¥æœ«åˆ‡
        chunks = re.split(r"([ã€‚ï¼ï¼Ÿ!?]\s+|[.!?]\s+)", s)
        sentences = []
        for i in range(0, len(chunks), 2):
            piece = chunks[i]
            sep = chunks[i+1] if i+1 < len(chunks) else ""
            sent = (piece + sep).strip()
            if sent:
                sentences.append(sent)

        # å¦‚æœå‡ ä¹åˆ‡ä¸å‡ºæ¥ï¼ˆæ¯”å¦‚å‚è€ƒæ–‡çŒ®/è¡¨æ ¼ï¼‰ï¼Œå°±èµ°å¼±æ–­ç‚¹åˆ‡
        if len(sentences) <= 1:
            return _split_long_by_soft_breaks(s, target)

        out, cur = [], ""
        soft_limit = int(target * 1.25)  # å…è®¸ç•¥è¶…ï¼Œæ¢å–å¥å­å®Œæ•´

        for sent in sentences:
            if not cur:
                cur = sent
                continue

            # ä¼˜å…ˆä¿æŒæ•´å¥ï¼šæ²¡è¶…è¿‡ soft_limit å°±ç»§ç»­æ‹¼
            if len(cur) + 1 + len(sent) <= soft_limit:
                cur = cur + " " + sent
                continue

            out.append(cur.strip())
            cur = sent

        if cur:
            out.append(cur.strip())

        # å¦‚æœè¿˜æœ‰è¶…é•¿ï¼ˆå•å¥è¿‡é•¿ï¼‰ï¼Œå†åšå¼±æ–­ç‚¹åˆ‡ï¼Œæœ€åæ‰ç¡¬åˆ‡
        final = []
        for x in out:
            if len(x) <= soft_limit:
                final.append(x)
            else:
                final.extend(_split_long_by_soft_breaks(x, target))

        return [t for t in final if t]


    def _split_long_by_soft_breaks(x: str, target: int) -> List[str]:
        res = []
        start = 0
        hard_limit = int(target * 1.35)

        while start < len(x):
            end = min(len(x), start + hard_limit)
            window = x[start:end]

            # å…ˆæ‰¾å¼±æ–­ç‚¹ï¼ˆä¼˜å…ˆçº§ï¼š; : , å†ç©ºæ ¼ï¼‰
            cut = max(window.rfind(";"), window.rfind(":"), window.rfind(","))

            if cut < int(target * 0.6):  # å¼±æ–­ç‚¹å¤ªé å‰å°±æ‰¾ç©ºæ ¼
                cut = window.rfind(" ")

            if cut < int(target * 0.6):  # è¿˜æ˜¯æ²¡æœ‰åˆé€‚æ–­ç‚¹æ‰ç¡¬åˆ‡
                cut = min(len(window), target)

            res.append(window[:cut].strip())
            start += cut

        return [r for r in res if r]

    refined = []
    for p in parts:
        refined.extend(split_by_sentence(p))

    # Step D: å¤ªçŸ­çš„æ®µè½åˆå¹¶åˆ°ä¸Šä¸€æ®µï¼ˆæ›´å¥½è¯»ï¼‰
    merged = []
    for p in refined:
        if len(p) < min_len and merged:
            merged[-1] = (merged[-1] + " " + p).strip()
        else:
            merged.append(p)

    # Step E: æ¸…ç†å¤šä½™ç©ºæ ¼
    merged = [re.sub(r"\s+", " ", p).strip() for p in merged if p.strip()]
    heading_words = r"(ABSTRACT|INTRODUCTION|CONCLUSION|REFERENCES|BIBLIOGRAPHY|EXPERIENCE|ACKNOWLEDG(E)?MENTS?|METHOD(S)?|RESULTS?|DISCUSSION)"
    isolated_num_end = re.compile(r"(?:^|\s)(\d+(\.\d+)*)\.\s*$")  # e.g., "1." "2.3."
    heading_start = re.compile(rf"^\s*{heading_words}\b", re.I)

    fixed = []
    i = 0
    while i < len(merged):
        cur = merged[i]

        if fixed:
            prev = fixed[-1]
            m_num = isolated_num_end.search(prev)

            # æ¡ä»¶ï¼šä¸Šä¸€æ®µç»“å°¾æ˜¯ "1." è¿™ç§ç¼–å·
            if m_num:
                num = m_num.group(1) + "."
                prev_wo_num = prev[:m_num.start()].rstrip()

                # æ¡ä»¶Aï¼šä¸‹ä¸€æ®µä»¥ INTRODUCTION/ABSTRACT/... å¼€å¤´
                # æ¡ä»¶Bï¼šæˆ–è€…ä¸‹ä¸€æ®µæœ¬èº«ä»¥ä¸€ä¸ªæ ‡é¢˜å¼ç¼–å·å¼€å¤´ï¼ˆæ¯”å¦‚ "1 INTRODUCTION" / "1. INTRODUCTION"ï¼‰
                if heading_start.match(cur) or re.match(r"^\s*\d+(\.\d+)*\s+\S+", cur):
                    # æŠŠç¼–å·ä»ä¸Šä¸€æ®µæŒªåˆ°ä¸‹ä¸€æ®µ
                    fixed[-1] = prev_wo_num
                    cur = f"{num} {cur}".strip()

        fixed.append(cur)
        i += 1

    # æ¸…æ‰å¯èƒ½äº§ç”Ÿçš„ç©ºæ®µ
    merged = [p for p in fixed if p and p.strip()]
    merged = [
        format_paragraph_for_reading(p, break_on_numbering=True, break_on_sentence=True)
        for p in merged
    ]
    return merged

@app.get("/api/files/{file_id}/paragraphs", response_model=ParagraphResponse)
async def get_file_paragraphs(file_id: str):
    """
    è¿”å›å…¨æ–‡æ®µè½ï¼ˆç¨³å®šç‰ˆï¼‰ï¼š
    - å¯¹ PDFï¼šä¼˜å…ˆç›´æ¥ç”¨ pdf_service.extract_text()ï¼ˆå°±æ˜¯ä½  /api/pdf/extract-text ç”¨çš„é‚£å¥—ï¼‰
    - å†æŒ‰å­—æ•°åˆ‡æ®µï¼ˆå°½é‡å¥å­è¾¹ç•Œï¼‰ï¼Œé¿å…"åˆé•¿åˆä¹±/æ–­è¡Œç²˜è¿"
    """
    db = SessionLocal()
    try:
        record = db.query(FileTextStore).filter(FileTextStore.file_id == file_id).first()
        if not record:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")

        filename = record.original_filename or ""

        # ===== å…³é”®ä¿®å¤ï¼šPDF å¼ºåˆ¶èµ° pdf_service.extract_textï¼Œåˆ«ç”¨å†å² record.text =====
        pdf_path = UPLOAD_DIR / f"{file_id}.pdf"
        text = ""

        if pdf_path.exists():
            # è¿™é‡Œå°±æ˜¯ä½ ä¹‹å‰"æå–æ–‡æœ¬å¾ˆå¹²å‡€"çš„é‚£æ¡é“¾è·¯
            text = pdf_service.extract_text(str(pdf_path)) or ""
        else:
            # éPDFï¼šé€€å› DB text
            text = (record.text or "").strip()

        if not text.strip():
            raise HTTPException(status_code=404, detail="æœªèƒ½æå–åˆ°æœ‰æ•ˆæ–‡æœ¬")

        # å¯é€‰ï¼šæŠŠå¹²å‡€æ–‡æœ¬å­˜å› DBï¼ˆä¸‹æ¬¡ä¸ç”¨é‡å¤æŠ½ï¼‰
        # æ³¨æ„ï¼šåªåœ¨ pdf_path å­˜åœ¨æ—¶å†™å›ï¼Œé¿å…æ±¡æŸ“å…¶å®ƒç±»å‹
        if pdf_path.exists():
            record.text = text
            db.commit()

        paras = split_paragraphs_by_chars(text, target=900, min_len=350)

        return {
            "status": "success",
            "file_id": file_id,
            "filename": filename,
            "paragraphs": [{"index": i, "text": p} for i, p in enumerate(paras)]
        }
    finally:
        db.close()

# ======================
# åŒè¯­å¯¹ç…§-æ‰¹é‡ç¿»è¯‘
# ======================
@app.post("/api/translate/batch", response_model=BatchTranslateResponse)
async def translate_batch(req: BatchTranslateRequest):
    if not req.items:
        return {"status": "success", "translations": []}

    # æ€»å­—ç¬¦é™åˆ¶
    total_chars = sum(len(it.get("text", "")) for it in req.items)
    if total_chars > 60000:
        return {"status": "error", "message": "æ‰¹é‡å†…å®¹è¿‡å¤§ï¼Œè¯·åˆ†æ‰¹ç¿»è¯‘", "translations": []}

    sem = asyncio.Semaphore(5)  # å¹¶å‘é™åˆ¶ï¼Œé¿å…æŠŠ Qwen å‹çˆ†

    async def run_one(it: dict):
        async with sem:
            if req.type == "polish":
                out = await simple_llm.polish_academic_text(it.get("text", ""))
            else:
                out = await simple_llm.translate_academic_text(it.get("text", ""))
            return {"index": it.get("index", 0), "translation": out}

    try:
        results = await asyncio.gather(*[run_one(it) for it in req.items])
        # ä¿è¯æŒ‰ index æ’åºè¿”å›
        results.sort(key=lambda x: x["index"])
        return {"status": "success", "translations": results}
    except Exception as e:
        logger.error(f"Batch translate failed: {e}", exc_info=True)
        return {"status": "error", "message": str(e), "translations": []}

# ======================
# åé¦ˆä¸è¯„ä¼°æ¥å£ (New)
# ======================

class FeedbackRequest(BaseModel):
    file_id: str = None
    question: str
    answer: str
    rating: int  # 1: Good, -1: Bad
    comment: str = None

@app.post("/feedback")
async def submit_feedback(feedback: FeedbackRequest):
    db = SessionLocal()
    try:
        new_feedback = Feedback(
            id=str(uuid.uuid4()),
            file_id=feedback.file_id,
            question=feedback.question,
            answer=feedback.answer,
            rating=feedback.rating,
            comment=feedback.comment
        )
        db.add(new_feedback)
        db.commit()
        return {"message": "Feedback received"}
    except Exception as e:
        logger.error(f"Error saving feedback: {e}")
        raise HTTPException(status_code=500, detail="Failed to save feedback")
    finally:
        db.close()

# è¾…åŠ©æµ‹è¯•æ¥å£ï¼šæ‰‹åŠ¨è§¦å‘è¯„ä¼°
class EvaluateRequest(BaseModel):
    question: str
    answer: str
    context: str

@app.post("/evaluate")
async def evaluate_answer(req: EvaluateRequest, qa_id: str = Query(None, description="QAè®°å½•ID")): # å¢åŠ qa_idå‚æ•°æ”¯æŒæŒä¹…åŒ–
    result = await evaluate_rag_response(req.question, req.answer, req.context)
    
    # å¦‚æœæä¾›äº†qa_idï¼Œåˆ™æ›´æ–°æ•°æ®åº“
    if qa_id:
        db = SessionLocal()
        try:
             qa_record = db.query(QAHistory).filter(QAHistory.id == qa_id).first()
             if qa_record:
                 qa_record.set_evaluation_dict(result) # ä½¿ç”¨ property setter
                 db.commit()
                 logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“ QA ID: {qa_id}")
        except Exception as e:
            logger.error(f"ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {str(e)}")
            # ä¸é˜»æ–­è¿”å›
        finally:
            db.close()
            
    return result

# ======================
# Q&A å†å²è®°å½•ç®¡ç† API (New)
# ======================

@app.get("/api/qa-history/{file_id}")
async def get_qa_history(file_id: str, db: Session = Depends(get_db)):
    """è·å–æŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰é—®ç­”å†å²"""
    try:
        history_list = db.query(QAHistory).filter(QAHistory.file_id == file_id).order_by(QAHistory.created_at.desc()).all()
        
        result = []
        for item in history_list:
            result.append({
                "id": item.id,
                "question": item.question,
                "answer": item.answer,
                "evidence": item.evidence_list, # ä½¿ç”¨ property getter
                "evaluation": item.evaluation_dict, # ä½¿ç”¨ property getter
                "created_at": item.created_at
            })
        return {"status": "success", "data": result}
    except Exception as e:
        logger.error(f"è·å–é—®ç­”å†å²å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–é—®ç­”å†å²å¤±è´¥: {str(e)}")

@app.delete("/api/qa-history/{qa_id}")
async def delete_qa_history(qa_id: str, db: Session = Depends(get_db)):
    """åˆ é™¤æŒ‡å®šçš„é—®ç­”è®°å½•"""
    try:
        item = db.query(QAHistory).filter(QAHistory.id == qa_id).first()
        if not item:
            raise HTTPException(status_code=404, detail="Q&A record not found")
        
        db.delete(item)
        db.commit()
        return {"status": "success", "message": "Deleted successfully"}
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"åˆ é™¤é—®ç­”è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤é—®ç­”è®°å½•å¤±è´¥: {str(e)}")

# ======================
# å¯åŠ¨å…¥å£ï¼ˆç”¨äºæœ¬åœ°è°ƒè¯•ï¼‰
# ======================
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)