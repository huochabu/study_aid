# agents/agent_team.py
from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager
import asyncio
try:
    import nest_asyncio
    nest_asyncio.apply()
except ImportError:
    pass
except Exception:
    pass
import os
from dotenv import load_dotenv

# è·å–é¡¹ç›®æ ¹ç›®å½•
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(BASE_DIR))

import logging
import re
from services.llm import simple_llm

logger = logging.getLogger(__name__)

load_dotenv(os.path.join(PROJECT_ROOT, ".env"))

DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")
if not DASHSCOPE_API_KEY:
    logger.warning("æœªé…ç½® DASHSCOPE_API_KEYï¼ŒAgentåŠŸèƒ½å°†ä¸å¯ç”¨ï¼Œä½†å­¦ä¹ ç³»ç»ŸåŠŸèƒ½ä»ç„¶å¯ç”¨")
    # ä¸ºäº†è®©ä»£ç èƒ½å¤Ÿç»§ç»­æ‰§è¡Œï¼Œè®¾ç½®ä¸€ä¸ªé»˜è®¤å€¼
    DASHSCOPE_API_KEY = ""

llm_config = {
    "config_list": [{
        "model": "qwen-max",
        "api_key": DASHSCOPE_API_KEY,
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "price": [0.004, 0.012] # [FIX] Add dummy price to silence "Model not found" warning

    }],
    "temperature": 0.7,
    # é™åˆ¶æ¯ä¸ªAgentçš„æœ€å¤§å›å¤ä»¤ç‰Œæ•°
    "max_tokens": 2000,
}

async def simple_extract_keywords(text, top_k=3):
    """
    Use LLM to extract search keywords for better accuracy.
    Falls back to heuristics if LLM fails.
    """
    try:
        # [RESTORED] User requested 20,000 char limit.
        # This balances deep context with performance.
        preview = text[:20000]

        prompt = f"""Extract 3-5 technical keywords/phrases from this text for search and analytics.
        Format: Comma separated list. No numbering.
        Text: {preview}"""
        
        response = await simple_llm.chat_completion([{"role": "user", "content": prompt}])
        if response and "Error" not in response:
            # Clean response
            keywords = [k.strip() for k in response.split(',') if k.strip()]
            if keywords:
                return keywords[:top_k]
    except Exception as e:
        logger.error(f"LLM keyword extraction failed: {e}")

    # Fallback to simple heuristic
    lines = [l.strip() for l in text.split('\n') if l.strip()]
    if not lines:
        return []
    
    potential_title = lines[0]
    clean_title = re.sub(r'[^\w\s]', '', potential_title)
    words = clean_title.split()
    return words[:top_k]

def create_agents():
    """å·¥å‚å‡½æ•°ï¼šæ¯æ¬¡åˆ›å»ºæ–° Agent å®ä¾‹ï¼Œé¿å…çŠ¶æ€æ±¡æŸ“"""
    log_expert = AssistantAgent(
        name="æ—¥å¿—ä¸“å®¶",
        system_message="ä½ æ˜¯ä¸€åæ—¥å¿—åˆ†æä¸“å®¶ã€‚è¯·ä»æ—¥å¿—ä¸­æå–é”™è¯¯ã€IPã€æ—¶é—´ï¼Œå¹¶æ¨æ–­æ ¹å› ã€‚",
        llm_config=llm_config, is_termination_msg=lambda x: "TERMINATE" in str(x.get("content", "")),
        code_execution_config=False
    )

    config_expert = AssistantAgent(
        name="é…ç½®ä¸“å®¶",
        system_message="ä½ æ“…é•¿ä»æŠ€æœ¯æ–‡æ¡£ä¸­æŠ½å–å‚æ•°ã€ä¾èµ–å…³ç³»ï¼Œæ„å»ºç»“æ„åŒ–é…ç½®çŸ¥è¯†ã€‚",
        llm_config=llm_config, is_termination_msg=lambda x: "TERMINATE" in str(x.get("content", "")),
        code_execution_config=False
    )

    explainer = AssistantAgent(
        name="è§£é‡Šä¸“å®¶",
        system_message="""ä½ è´Ÿè´£æ•´åˆå…¶ä»–ä¸“å®¶çš„è¾“å‡ºï¼Œç”Ÿæˆæ¸…æ™°ã€ç»“æ„åŒ–çš„æ€»ç»“ã€‚
        
        ä½ çš„è¾“å‡ºå¿…é¡»åŒ…å«ä»¥ä¸‹å›ºå®šç« èŠ‚ï¼š
        ### 1. æ·±åº¦åˆ†ææ€»ç»“
        (è¿™é‡Œæ˜¯å¯¹å…¨æ–‡çš„è¯¦ç»†é€»è¾‘æ€»ç»“)
        
        ### æ ‘å½¢æ€ç»´å¯¼å›¾æ–‡æœ¬æè¿°
        (è¿™é‡Œä½¿ç”¨ Markdown åˆ—è¡¨å½¢å¼è¾“å‡ºé€»è¾‘ç»“æ„ï¼Œå‰ç«¯å°†æ®æ­¤ç”Ÿæˆæ€ç»´å¯¼å›¾)
        
        æ³¨æ„ï¼šåœ¨æ‰€æœ‰å†…å®¹è¾“å‡ºå®Œæ¯•åï¼Œå¿…é¡»å¦èµ·ä¸€è¡Œè¾“å‡ºï¼šTERMINATE""",
        llm_config=llm_config, is_termination_msg=lambda x: "TERMINATE" in str(x.get("content", "")),
        code_execution_config=False
    )

    paper_expert = AssistantAgent(
        name="å­¦æœ¯é˜…è¯»ä¸“å®¶",
        system_message="""ä½ æ˜¯ä¸€åå°½èŒçš„å­¦æœ¯åŠ©ç†ã€‚ä½ çš„ä»»åŠ¡æ˜¯æå–è®ºæ–‡çš„æ ¸å¿ƒä¿¡æ¯ï¼Œä¾›åç»­ä¸“å®¶ä½¿ç”¨ã€‚
        è¯·ç”¨**ç®€æ´çš„å­¦æœ¯è¯­è¨€**åˆ—å‡ºä»¥ä¸‹è¦ç‚¹ï¼ˆä¸¥ç¦é•¿ç¯‡å¤§è®ºï¼‰ï¼š
        1. **åˆ›æ–°ç‚¹ (Novelty)**ï¼šä¸€å¥è¯æ¦‚æ‹¬æ ¸å¿ƒè´¡çŒ®ã€‚
        2. **æ–¹æ³•è®º (Methodology)**ï¼šç®€è¿°æ ¸å¿ƒç®—æ³•/æ¶æ„ã€‚
        3. **å®éªŒç»“æœ (Results)**ï¼šåˆ—å‡ºå…³é”®æŒ‡æ ‡(SOTAå¯¹æ¯”)ã€‚
        
        ã€ä¸¥é‡è­¦å‘Šã€‘
        âŒ **ä¸¥ç¦**è¾“å‡º "æ€ç»´å¯¼å›¾"ã€"çŸ¥è¯†å›¾è°±" æˆ– "æ ¸å¿ƒå…³é”®è¯" ç« èŠ‚ã€‚
        âŒ ä½ çš„ä»»åŠ¡ä»…ä»…æ˜¯æä¾›**ç´ æ**ï¼Œæ€»ç»“å’Œç»˜å›¾å·¥ä½œå®Œå…¨ç”±åç»­çš„è§£é‡Šä¸“å®¶å®Œæˆã€‚
        âŒ ä¿æŒå®¢è§‚ç®€æ´ï¼Œä¸è¦è¿›è¡Œè¿‡åº¦è§£è¯»ã€‚""",
        llm_config=llm_config, is_termination_msg=lambda x: "TERMINATE" in str(x.get("content", "")),
        code_execution_config=False
    )

    general_expert = AssistantAgent(
        name="é€šç”¨çŸ¥è¯†ä¸“å®¶",
        system_message="""ä½ æ˜¯ä¸€ååšå­¦çš„è¯»ä¹¦é¡¾é—®ã€‚è¯·å¯¹ä¹¦ç±/é•¿æ–‡æœ¬è¿›è¡Œæ·±åº¦é˜…è¯»ï¼š
        1. åˆ†ç« æ‘˜è¦ (Chapter Summary)ï¼šæŒ‰é€»è¾‘æ®µè½æ€»ç»“æ ¸å¿ƒå†…å®¹ã€‚
        2. æ¦‚å¿µæå– (Key Concepts)ï¼šè¯†åˆ«å¹¶è§£é‡Šæ–‡ä¸­çš„æ ¸å¿ƒæ¦‚å¿µï¼ˆå¦‚æœ¯è¯­ã€ç†è®ºï¼‰ã€‚
        3. é€»è¾‘æ¢³ç† (Logical Flow)ï¼šæ¢³ç†ä½œè€…çš„è®ºè¯é€»è¾‘æˆ–å™äº‹çº¿ç´¢ã€‚""",
        llm_config=llm_config, is_termination_msg=lambda x: "TERMINATE" in str(x.get("content", "")),
        code_execution_config=False
    )

    critic = AssistantAgent(
        name="è´¨æ£€å‘˜",
        system_message="""ä½ æ˜¯ä¸€åå‹å¥½çš„å†…å®¹æ£€æŸ¥å‘˜ã€‚
        
        ä½ çš„ä»»åŠ¡æ˜¯ç¡®ä¿ä¸“å®¶è¾“å‡ºäº†æ ¸å¿ƒå†…å®¹ï¼ˆå¦‚åˆ›æ–°ç‚¹ã€æ–¹æ³•ç­‰ï¼‰ï¼Œè€Œ**ä¸å¿…**è¿‡äºçº ç»“ç»†èŠ‚æˆ–æ·±åº¦ã€‚
        é™¤éå†…å®¹å®Œå…¨ç¦»é¢˜æˆ–ç©ºç™½ï¼Œå¦åˆ™è¯·å°½é‡**æ”¾è¡Œ**ã€‚

        ã€æ£€æŸ¥æ ‡å‡†ã€‘
        - æ˜¯å¦æœ‰åŸºæœ¬çš„å†…å®¹è¾“å‡ºï¼Ÿ
        - æ ¼å¼æ˜¯å¦å¤§è‡´æ¸…æ™°ï¼Ÿ
        
        ã€è¾“å‡ºè§„åˆ™ã€‘
        - åªè¦ä¸æ˜¯ä¸¥é‡é”™è¯¯ï¼Œè¯·ç›´æ¥å›å¤ï¼š"APPROVE"ã€‚
        - å¦‚æœç¡®å®ç¼ºå¤±æ ¸å¿ƒå†…å®¹ï¼Œè¯·ç”¨ä¸€å¥è¯æŒ‡å‡ºã€‚
        """,
        llm_config=llm_config, is_termination_msg=lambda x: "TERMINATE" in str(x.get("content", "")),
        code_execution_config=False
    )

    user_proxy = UserProxyAgent(
        name="ç”¨æˆ·ä»£ç†",
        human_input_mode="NEVER",
        max_consecutive_auto_reply=5, is_termination_msg=lambda x: "TERMINATE" in str(x.get("content", "")),
        code_execution_config={"use_docker": False}
    )
    return user_proxy, log_expert, config_expert, explainer, paper_expert, general_expert, critic



class StreamingGroupChat(GroupChat):
    def __init__(self, agents, messages, max_round=10, speaker_selection_method="auto", allow_repeat_speaker=True, callback=None):
        super().__init__(agents, messages, max_round, speaker_selection_method, allow_repeat_speaker)
        self.callback = callback
        self.explainer_has_spoken = False 

    def _get_agent_by_name(self, name):
        for agent in self.agents:
            if agent.name == name:
                return agent
        return None

    def select_speaker(self, last_speaker, selector):
        """Extremely aggressive speaker selection and termination."""
        
        # [NUCLEAR OPTION] Scan entire message history for ANY message from Explainer
        # This is the most robust way to ensure we never speak twice.
        explainer_detected = False
        for msg in self.messages:
            if "è§£é‡Š" in str(msg.get("name", "")):
                explainer_detected = True
                break
        
        # Also check current flags and last speaker
        should_terminate = (
            explainer_detected or 
            self.explainer_has_spoken or 
            "è§£é‡Š" in last_speaker.name
        )
        
        # Content based check
        if self.messages:
            if "TERMINATE" in str(self.messages[-1].get("content", "")):
                should_terminate = True

        if should_terminate:
            print(f"DEBUG: TERMINATING chat. Explainer detected: {explainer_detected}, has_spoken: {self.explainer_has_spoken}, last_speaker: {last_speaker.name}")
            logger.info(f"ğŸ›‘ [FORCE TERMINATE] Explainer has spoken or TERMINATE found. Ending GroupChat.")
            return None

        # [EXPLICIT STATE MACHINE]
        print(f"DEBUG: Turn for: {last_speaker.name}. Selection in progress.")
        logger.info(f"ğŸ—£ï¸ [StreamingGroupChat] Turn for: {last_speaker.name}")
        
        expert_names = ["å­¦æœ¯é˜…è¯»ä¸“å®¶", "æ—¥å¿—ä¸“å®¶", "é…ç½®ä¸“å®¶", "é€šç”¨çŸ¥è¯†ä¸“å®¶"]
        
        # 1. Expert -> Critic
        if any(name in last_speaker.name for name in expert_names):
             return self._get_agent_by_name("è´¨æ£€å‘˜")
             
        # 2. Critic -> Explainer (if APPROVE) or back to Expert
        if "è´¨æ£€å‘˜" in last_speaker.name:
            last_content = self.messages[-1].get("content", "") if self.messages else ""
            if "APPROVE" in last_content:
                return self._get_agent_by_name("è§£é‡Šä¸“å®¶")
            else:
                if len(self.messages) >= 2:
                    target_name = self.messages[-2].get("name", "")
                    target_agent = self._get_agent_by_name(target_name)
                    if target_agent: return target_agent
                return self._get_agent_by_name("è§£é‡Šä¸“å®¶") # fallback

        # 3. Explainer -> STOP (Secondary check)
        if "è§£é‡Š" in last_speaker.name:
            return None

        # 4. DEFAULT Fallback to AutoGen's internal logic
        # Note: Do NOT set speaker_selection_method=self.select_speaker elsewhere, 
        # as calling super().select_speaker would cause recursion.
        return super().select_speaker(last_speaker, selector)

    def append(self, message: dict, speaker: AssistantAgent):
        message['name'] = speaker.name
        if "è§£é‡Š" in speaker.name:
            self.explainer_has_spoken = True
            logger.info("ğŸš© Explainer flag set in append.")
            
        super().append(message, speaker)
        
        if self.callback:
            try:
                content = message.get("content", "")
                if not content: return
                clean_content = content.replace("TERMINATE", "").strip()
                if not clean_content: return
                msg_to_send = message.copy()
                msg_to_send['content'] = clean_content 
                msg_to_send['name'] = speaker.name
                self.callback(msg_to_send)
            except Exception as e:
                logger.error(f"Callback failed: {e}")


async def analyze_with_agents(input_text: str, agent_types: list, **kwargs) -> dict:
    callback = kwargs.get("callback")
    """
    å¯åŠ¨å¤šæ™ºèƒ½ä½“åä½œåˆ†æ (Restored Autogen Version)
    :param input_text: è¾“å…¥æ–‡æœ¬
    :param agent_types: å¦‚ ["log", "config"]
    :return: ç»“æ„åŒ–ç»“æœ
    """
    logger.info(f"ğŸš€ Starting Multi-Agent analysis with Autogen. Input length: {len(input_text)}")
    
    # æ£€æŸ¥æ˜¯å¦é…ç½®äº†DASHSCOPE_API_KEY
    if not DASHSCOPE_API_KEY:
        logger.warning("æœªé…ç½® DASHSCOPE_API_KEYï¼Œè·³è¿‡Agentåˆ†æ")
        return {
            "summary": "Agentåˆ†æåŠŸèƒ½éœ€è¦é…ç½® DASHSCOPE_API_KEY",
            "reasoning_steps": ["è·³è¿‡Agentåˆ†æï¼šæœªé…ç½®APIå¯†é’¥"],
            "agents_involved": []
        }
    
    try:
        # 1. å‡†å¤‡å¤šæ™ºèƒ½ä½“ç¯å¢ƒ
        user_proxy, log_expert, config_expert, explainer, paper_expert, general_expert, critic = create_agents()
        
        # 2. æ ¹æ® file_type / agent_types ç­›é€‰å‚ä¸çš„ Agent
        participants = []
        
        if "academic" in agent_types:
            participants = [user_proxy, paper_expert, critic, explainer]
            # ç»™å­¦æœ¯ä¸“å®¶çš„ç‰¹å®šæŒ‡ä»¤
            initial_instruction = "è¯·å¯¹è¿™ç¯‡è®ºæ–‡è¿›è¡Œæ·±åº¦åˆ†æã€‚å…ˆç”± @å­¦æœ¯é˜…è¯»ä¸“å®¶ è¿›è¡Œç»“æ„åŒ–æ‹†è§£ï¼Œç„¶åç”± @è´¨æ£€å‘˜ å®¡æŸ¥ï¼Œæœ€åç”± @è§£é‡Šä¸“å®¶ æ€»ç»“ã€‚"
        elif "log" in agent_types:
            participants = [user_proxy, log_expert, critic, explainer]
            initial_instruction = "è¯·åˆ†æè¿™æ®µæ—¥å¿—ã€‚å…ˆç”± @æ—¥å¿—ä¸“å®¶ æå–å…³é”®é”™è¯¯å’Œæ ¹å› ï¼Œç„¶åç”± @è´¨æ£€å‘˜ å®¡æŸ¥ï¼Œæœ€åç”± @è§£é‡Šä¸“å®¶ æ€»ç»“ã€‚"
        elif "config" in agent_types:
            participants = [user_proxy, config_expert, critic, explainer]
            initial_instruction = "è¯·åˆ†æè¿™ä»½é…ç½®/ä»£ç æ–‡æ¡£ã€‚å…ˆç”± @é…ç½®ä¸“å®¶ æå–å‚æ•°å’Œä¾èµ–ï¼Œç„¶åç”± @è´¨æ£€å‘˜ å®¡æŸ¥ï¼Œæœ€åç”± @è§£é‡Šä¸“å®¶ æ€»ç»“ã€‚"
        elif "book" in agent_types:
            participants = [user_proxy, general_expert, critic, explainer]
            initial_instruction = "è¯·å¯¹è¿™æœ¬ä¹¦ç±/é•¿æ–‡æœ¬è¿›è¡Œæ·±åº¦åˆ†æã€‚å…ˆç”± @é€šç”¨çŸ¥è¯†ä¸“å®¶ æå–ç« èŠ‚æ‘˜è¦å’Œæ ¸å¿ƒæ¦‚å¿µï¼Œç„¶åç”± @è´¨æ£€å‘˜ å®¡æŸ¥ï¼Œæœ€åç”± @è§£é‡Šä¸“å®¶ æ€»ç»“ã€‚"
        else:
            participants = [user_proxy, general_expert, critic, explainer]
            initial_instruction = "è¯·å¯¹è¿™ä»½æ–‡æ¡£è¿›è¡Œæ·±åº¦åˆ†æã€‚å…ˆç”± @é€šç”¨çŸ¥è¯†ä¸“å®¶ åˆ†ææ ¸å¿ƒå†…å®¹ï¼Œç„¶åç”± @è´¨æ£€å‘˜ å®¡æŸ¥ï¼Œæœ€åç”± @è§£é‡Šä¸“å®¶ æ€»ç»“ã€‚"

        # 3. Web Search Augmentation (Native Aliyun Search)
        web_context = ""
        try:
             # Use LLM to extract smart keywords
             keywords = await simple_extract_keywords(input_text)
             if keywords:
                 query = " ".join(keywords[:3]) + " latest trends 2024 2025"
                 logger.info(f"ğŸ•¸ï¸ [WebSearch-Native] Triggering Aliyun Search for: {query}")
                 
                 from utils.qwen_client import async_call_qwen
                 # Call Qwen with built-in search enabled
                 search_prompt = f"Please search the internet for the latest information (2024-2025) regarding: {query}. Summarize the key findings, new technologies, and future trends."
                 web_context = await async_call_qwen(search_prompt, enable_search=True)
                 
                 if web_context:
                    logger.info("âœ… [WebSearch-Native] Search successful.")
                 else:
                    logger.warning("âš ï¸ [WebSearch-Native] Search returned empty.")
             else:
                 logger.info("ğŸ•¸ï¸ [WebSearch] No keywords extracted.")
        except Exception as e:
             logger.warning(f"ğŸ•¸ï¸ [WebSearch] Failed: {e}")

        # 4. æ„å»ºä»»åŠ¡æ¶ˆæ¯
        task_msg = f"""
        {initial_instruction}

        å¾…åˆ†ææ–‡æœ¬ï¼š
        {input_text}

        """
        
        if web_context:
            task_msg += f"\n\nå‚è€ƒçš„ç½‘ç»œèƒŒæ™¯ä¿¡æ¯ï¼ˆè¯·åŸºäºæ­¤ä¿¡æ¯è¡¥å……ã€å‰æ²¿å‘å±•ã€‘ç« èŠ‚ï¼‰ï¼š\n{web_context}\n"

        task_msg += """
        
        ã€é‡è¦æ£€æŸ¥ç‚¹ã€‘
        è¯· @è§£é‡Šä¸“å®¶ åœ¨æœ€ç»ˆæ€»ç»“æ—¶ï¼Œå¿…é¡»åŒ…å«ä¸€ä¸ªåä¸º "### æ ‘å½¢æ€ç»´å¯¼å›¾æ–‡æœ¬æè¿°" çš„ç« èŠ‚ã€‚
        è¯¥ç« èŠ‚å†…å®¹å¿…é¡»æ˜¯ **æœ‰æ„ä¹‰çš„é¢†åŸŸå†…å®¹**ï¼Œä¸¥ç¦ä½¿ç”¨ "åˆ†æ”¯1"ã€"å­èŠ‚ç‚¹A" ç­‰æ— æ„ä¹‰å ä½ç¬¦ã€‚
        æ ¼å¼è¦æ±‚ä¸ºä¸¥æ ¼çš„ Markdown åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š
        ### æ ‘å½¢æ€ç»´å¯¼å›¾æ–‡æœ¬æè¿°
        ã€æµç¨‹è¦æ±‚ã€‘
        1. é¦–å…ˆç”±ç›¸å…³é¢†åŸŸçš„ä¸“å®¶ï¼ˆå¦‚æ—¥å¿—ä¸“å®¶ã€å­¦æœ¯ä¸“å®¶ï¼‰å‘è¡¨è§‚ç‚¹ã€‚
        2. æœ€åç”± @è§£é‡Šä¸“å®¶ è¿›è¡Œæ€»ç»“ã€‚

        ã€è§£é‡Šä¸“å®¶è¾“å‡ºè§„èŒƒã€‘
        @è§£é‡Šä¸“å®¶ è¯·åŠ¡å¿…æŒ‰ç…§ä»¥ä¸‹ä¸¤éƒ¨åˆ†é¡ºåºè¾“å‡ºï¼Œå„éƒ¨åˆ†ä¹‹é—´ç”¨åˆ†å‰²çº¿ "---" éš”å¼€ï¼š

        **ç¬¬ä¸€éƒ¨åˆ†ï¼šæ·±åº¦åˆ†ææ€»ç»“**
        è¯·è¯¦ç»†é˜è¿°èƒŒæ™¯ã€é—®é¢˜åŸå› ã€è§£å†³æ–¹æ¡ˆæˆ–æ ¸å¿ƒè§‚ç‚¹ã€‚è¿™éƒ¨åˆ†æ˜¯ä½ ä¹‹å‰çš„"æ€è€ƒè¿‡ç¨‹"ï¼Œè¯·ä¿ç•™å¹¶å……å®ã€‚

        ---

        **ç¬¬äºŒéƒ¨åˆ†ï¼šæ ‘å½¢æ€ç»´å¯¼å›¾æ–‡æœ¬æè¿°**
        è¯·åŒ…å«ä¸€ä¸ªåä¸º "### æ ‘å½¢æ€ç»´å¯¼å›¾æ–‡æœ¬æè¿°" çš„ç« èŠ‚ã€‚
        è¯¥ç« èŠ‚å†…å®¹å¿…é¡»æ˜¯ **æœ‰æ„ä¹‰çš„é¢†åŸŸå†…å®¹**ï¼Œä¸¥ç¦ä½¿ç”¨ "åˆ†æ”¯1"ã€"å­èŠ‚ç‚¹A" ç­‰æ— æ„ä¹‰å ä½ç¬¦ã€‚
        æ ¼å¼è¦æ±‚ä¸ºä¸¥æ ¼çš„ Markdown åˆ—è¡¨ã€‚
        
        ---

        **ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ ¸å¿ƒå…³é”®è¯**
        è¯·è¾“å‡ºä¸€ä¸ªåä¸º "### æ ¸å¿ƒå…³é”®è¯" çš„ç« èŠ‚ã€‚
        è¯·ä»…åˆ—å‡º **3-5ä¸ª** æœ€å…·ä»£è¡¨æ€§çš„æ ¸å¿ƒé¢†åŸŸè¯æ±‡ï¼Œç”¨è‹±æ–‡é€—å· separatingï¼Œä¾‹å¦‚ï¼š
        ### æ ¸å¿ƒå…³é”®è¯
        TCPæ‹¥å¡æ§åˆ¶, ACKåˆ†å‰²æ”»å‡», ç½‘ç»œå®‰å…¨, æµé‡æ•´å½¢

        ---

        **ç¬¬å››éƒ¨åˆ†ï¼šå‰æ²¿å‘å±•ä¸æœªæ¥è¶‹åŠ¿ (åŸºäºè”ç½‘ä¿¡æ¯)**
        è¯·è¾“å‡ºä¸€ä¸ªåä¸º "### å‰æ²¿å‘å±•ä¸æœªæ¥è¶‹åŠ¿" çš„ç« èŠ‚ã€‚
        ç»“åˆæä¾›çš„ã€ç½‘ç»œèƒŒæ™¯ä¿¡æ¯ã€‘ï¼Œç®€è¦åˆ†æè¯¥æŠ€æœ¯/ä¸»é¢˜åœ¨å½“å‰ï¼ˆ2024-2025ï¼‰çš„æœ€æ–°è¿›å±•ã€æ–°çš„è§£å†³æ–¹æ¡ˆæˆ–æœªæ¥çš„æ¼”è¿›æ–¹å‘ã€‚
        å¦‚æœç½‘ç»œä¿¡æ¯ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†åº“è¿›è¡Œåˆç†æ¨æ¼”ã€‚
        
        è¯·ç¡®ä¿ä¸¥æ ¼æŒ‰ç…§æ­¤ç»“æ„è¾“å‡ºã€‚
        """

        # 5. Initialize Custom GroupChat
        groupchat = StreamingGroupChat(
            agents=participants, 
            messages=[], 
            max_round=12,
            speaker_selection_method="auto", # Use inheritance to override select_speaker
            callback=callback
        )
        
        # DO NOT set groupchat.speaker_selection_method = groupchat.select_speaker
        # That would cause infinite recursion when select_speaker calls super().

        
        manager = GroupChatManager(
            groupchat=groupchat, 
            llm_config=llm_config
        )

        # 6. å¯åŠ¨å¯¹è¯ (Async)
        logger.info("Chat initiated...")
        chat_result = await user_proxy.a_initiate_chat(
            manager,
            message=task_msg
        )
        
        # 7. æå–ç»“æœ
        # The last message usually contains the summary from Explainer
        summary = "åˆ†æå®Œæˆ"

        explainer_msgs = [
            msg.get("content", "").replace("TERMINATE", "").strip() 
            for msg in groupchat.messages 
            if "è§£é‡Š" in msg.get("name", "")
        ]
        explainer_msgs = [m for m in explainer_msgs if m] # Remove empty
        if explainer_msgs:
            summary = explainer_msgs[-1]
        else:
            summary = groupchat.messages[-1].get("content", "").replace("TERMINATE", "").strip()




        reasoning_steps = [msg.get("content", "") for msg in groupchat.messages]
        agents_involved = [agent.name for agent in participants if agent.name != "ç”¨æˆ·ä»£ç†"]
        
        logger.info("âœ… Multi-Agent analysis completed successfully.")
        
        result = {
            "summary": summary,
            "reasoning_steps": reasoning_steps,
            "agents_involved": agents_involved,
            "agent_types": agent_types
        }
        return result

    except Exception as e:
        logger.error(f"âŒ Multi-agent analysis failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {"error": f"å¤šæ™ºèƒ½ä½“åˆ†æå¤±è´¥: {str(e)}"}
